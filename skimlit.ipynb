{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![title](img/skimlit.png)\n",
    "\n",
    "\n",
    "<a style=\"margin: 0 5px\" href=\"https://colab.research.google.com/drive/1iceHKxffz_oLud9Zoxu300FZKANyI6kx?usp=sharing\"><img    src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
    "\n",
    "## üìñ The purpose of SkimLit is to build a NLP model to make reading medical abstracts easier.\n",
    "\n",
    "The paper being replicated is available [here](https://arxiv.org/abs/1710.06071) <br>\n",
    "The dataset can be found [here](https://github.com/Franck-Dernoncourt/pubmed-rct.git)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### ‚¨áÔ∏è Download the <span style=\"color:green\">data</span>.\n",
    "Start with the smaller, 20k Dataset. Once a model that performs best with the 20k dataset is found, upscale the model to train on the 200k dataset "
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "\n",
    "# If file doesn't already exist, download the dataset\n",
    "if os.path.exists('./data/pubmed-rct'):\n",
    "    print(\"File already exists\")\n",
    "    \n",
    "else:\n",
    "    !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# List files inside the PubMed_20k dataset\n",
    "!ls data/pubmed-rct/pubmed_20k_rct_numbers_replaced_with_at_sign/\n",
    "!ls data/pubmed-rct/pubmed_20k_rct/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dev.txt   test.txt  train.txt\n",
      "dev.txt   test.txt  train.txt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Start with the smaller dataset, with numbers replaced with \"@\" sign\n",
    "data_dir = \"data/pubmed-rct/pubmed_20k_rct_numbers_replaced_with_at_sign/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "<h1 align=\"center\">ü§ñ Preprocess Data</h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Create a function that reads the lines of a document\n",
    "def get_lines(filename):\n",
    "    \"\"\"\n",
    "    Reads filename (a text file) and returns the lines of text as a list.\n",
    "\n",
    "    Args:\n",
    "      filename: a string containing the target filepath to read.\n",
    "\n",
    "    Returns:\n",
    "      A list of strings with one string per line from the target filename.\n",
    "      For example:\n",
    "      [\"this is the first line of filename\",\n",
    "       \"this is the second line of filename\",\n",
    "       \"...\"]\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Read the lines of the training dataset, into a variable\n",
    "train_lines = get_lines(data_dir + \"train.txt\") \n",
    "\n",
    "# Show the first 10 lines\n",
    "train_lines[:10] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(f'There are {len(train_lines)} training lines')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 210040 training lines\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Format\n",
    "Load data into python dictionaries, this adds structure to the dataset that the text file doesn't have\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'line_number': 0,\n",
    "        'target': 'BACKGROUND',\n",
    "        'text': 'Emotional eating is associated with overeating and the development of obesity .\\n',\n",
    "        'total_lines': 11\n",
    "    },\n",
    "\n",
    "    # Next instance of data\n",
    "    {\n",
    "        ......\n",
    "    }\n",
    "]\n",
    "```\n",
    "### Define a function to load data into the data structure shown above"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def preprocess_text(filename):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "    Takes in filename, reads its contents and sorts through each line,\n",
    "    extracting things like the target label, the text of the sentence,\n",
    "    how many sentences are in the current abstract and what sentence number\n",
    "    the target line is.\n",
    "\n",
    "    Args:\n",
    "      filename: a string of the target text file to read and extract line data\n",
    "      from.\n",
    "\n",
    "    Returns:\n",
    "      A list of dictionaries each containing a line from an abstract,\n",
    "      the lines label, the lines position in the abstract and the total number\n",
    "      of lines in the abstract where the line is from. For example:\n",
    "\n",
    "      [{\"target\": 'CONCLUSION',\n",
    "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
    "        \"line_number\": 8,\n",
    "        \"total_lines\": 8}]\n",
    "  \"\"\"\n",
    "\n",
    "    input_lines = get_lines(filename)\n",
    "    abstract_lines = \"\" # Create\n",
    "    abstract_samples = []\n",
    "\n",
    "    for line in input_lines:\n",
    "        \n",
    "        if line.startswith(\"###\"): # Check to see if the new line is the ID label\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # Reset the abstract string if the line is the ID\n",
    "\n",
    "        # A space indicates the start of another abstract, so we have to process the current abstract before moving\n",
    "        # a new abstract\n",
    "        elif line.isspace(): # Check if the new line is a space\n",
    "            \n",
    "            # Split the abstract into seperate lines\n",
    "            # the splitlines() function looks for a line break and splits the lines\n",
    "            abstract_line_split = abstract_lines.splitlines() # Returns a list of strings\n",
    "        \n",
    "            # Iterate through each line in the abstract\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                # Populate the dictionary with the line data\n",
    "                abstract_dict = {}\n",
    "\n",
    "                # Splits the target text and text\n",
    "                target_text_split = abstract_line.split('\\t')\n",
    "\n",
    "                # Populate the dictionary\n",
    "                abstract_dict['target'] = target_text_split[0]\n",
    "                abstract_dict['text'] = target_text_split[1].lower()\n",
    "                abstract_dict['line_number'] = abstract_line_number\n",
    "                abstract_dict['total_lines'] = len(abstract_line_split) - 1 # We want to start from 0\n",
    "\n",
    "                abstract_samples.append(abstract_dict)\n",
    "\n",
    "        # If neither of the above two criteria is fulfilled, the line is a target label / text line\n",
    "        # So just append the data into the abstract_lines string\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "        \n",
    "    return abstract_samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üññüèΩ Dataset Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%time\n",
    "\n",
    "# Get the data and preprocess it\n",
    "train_samples = preprocess_text(data_dir + 'train.txt')\n",
    "val_samples = preprocess_text(data_dir + 'dev.txt') # dev is another name for validation\n",
    "test_samples = preprocess_text(data_dir + 'test.txt')\n",
    "print(\"No. of training samples: {} \\nNo. of validation samples: {} \\nNo. of testing samples: {}\"\n",
    "        .format(len(train_samples), len(val_samples), len(test_samples))\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No. of training samples: 180040 \n",
      "No. of validation samples: 30212 \n",
      "No. of testing samples: 30135\n",
      "CPU times: user 458 ms, sys: 58.9 ms, total: 517 ms\n",
      "Wall time: 523 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_samples[:12] # Check the lines of the first abstract in the training abstract"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 11}]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Turn dictionaries into pandas DataFrame to better visualize it\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "# Show first 13 lines of data\n",
    "train_df.head(13)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         target                                               text  \\\n",
       "0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
       "1       METHODS  a total of @ patients with primary knee oa wer...   \n",
       "2       METHODS  outcome measures included pain reduction and i...   \n",
       "3       METHODS  pain was assessed using the visual analog pain...   \n",
       "4       METHODS  secondary outcome measures included the wester...   \n",
       "5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n",
       "6       RESULTS  there was a clinically relevant reduction in t...   \n",
       "7       RESULTS  the mean difference between treatment arms ( @...   \n",
       "8       RESULTS  further , there was a clinically relevant redu...   \n",
       "9       RESULTS  these differences remained significant at @ we...   \n",
       "10      RESULTS  the outcome measures in rheumatology clinical ...   \n",
       "11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n",
       "12   BACKGROUND  emotional eating is associated with overeating...   \n",
       "\n",
       "    line_number  total_lines  \n",
       "0             0           11  \n",
       "1             1           11  \n",
       "2             2           11  \n",
       "3             3           11  \n",
       "4             4           11  \n",
       "5             5           11  \n",
       "6             6           11  \n",
       "7             7           11  \n",
       "8             8           11  \n",
       "9             9           11  \n",
       "10           10           11  \n",
       "11           11           11  \n",
       "12            0           10  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>there was a clinically relevant reduction in t...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the mean difference between treatment arms ( @...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>further , there was a clinically relevant redu...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>these differences remained significant at @ we...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the outcome measures in rheumatology clinical ...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>emotional eating is associated with overeating...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "<h1 align=\"center\">üë®üèΩ‚Äçüíª Dataset Analysis</h3> \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Visualize the distribution in training data (according to no. of samples per target)\n",
    "\n",
    "train_df['target'].value_counts().plot(kind='barh')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Let's check the distribution of the length of lines\n",
    "train_df['total_lines'].plot.hist()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Get list of sentences</b> <br>\n",
    "The main inputs to the deep learning model, would be a list of strings which we can obtain via the function below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train_sentences = train_df['text'].tolist()\n",
    "val_sentences = val_df['text'].tolist()\n",
    "test_sentences = test_df['text'].tolist()\n",
    "\n",
    "len(train_sentences), len(val_sentences), len(test_sentences),"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "train_sentences[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "<h1 align=\"center\">üõ† Feature Engineering</h3> \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding\n",
    "ML models require numeric labels <br>\n",
    "<br>\n",
    "<b>Two methods</b>\n",
    "1. Label Encode (e.g `Methods -- 1, Results -- 2, ...`)\n",
    "2. One-hot encode (e.g `Methods -- [1,0,0,0,0], Results -- [0,1,0,0,0]`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> <b>What does `sparse` mean?</b> <br>\n",
    "A sparse array is one that contains mostly zeros and few non-zero entries\n",
    "\n",
    "> <b>What does `dense` mean?</b> <br>\n",
    "A dense array contains mostly non-zeros"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1Ô∏è‚É£ One-hot encode labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# One-hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse = False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df['target'].to_numpy().reshape(-1,1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df['target'].to_numpy().reshape(-1,1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df['target'].to_numpy().reshape(-1,1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import tensorflow as tf\n",
    "tf.constant(train_labels_one_hot)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-28 15:28:19.713744: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(180040, 5), dtype=float64, numpy=\n",
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2Ô∏è‚É£ Label encode labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "train_labels_encoded # These labels are label encoded"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Get class (reverse engineer to get the class names)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting a series of modelling experiments\n",
    "Trying out different models and seeing which one works best\n",
    "\n",
    "| Experiment | Model Name |\n",
    "| --- | --- |\n",
    "| 0 | TF-IDF Multinomial Naive Bayes Classifier (Baseline) |\n",
    "| 1 | Conv1D with token embeddings |\n",
    "| 2 | TensorFlow Hub Pretrained Feature Extractor |\n",
    "| 3 | Conv1D with character embeddings |\n",
    "| 4 | Pretrained token embeddings (same as 2) + character embeddings (same as 3) |\n",
    "| 5 | Pretrained token embeddings + character embeddings + positional embeddings |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "![title](img/model_0.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Naive Bayes Classifier</b>\n",
    "\n",
    "\n",
    "Bayes Theorem \n",
    "\n",
    "$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$\n",
    "\n",
    "If any two events are independent, then $P(A,B) = P(A)P(B)$\n",
    "\n",
    "Using the above two concepts, we can reach this result\n",
    "\n",
    "P(y|X) = $\\frac{P(X|y)P(y)}{P(X)}$ where X = (x_1, x_2, ..., x_n)\n",
    "\n",
    "\n",
    "P(y|x_1,...x_n) = $\\frac{P(x_1|y)...P(x_n|y)P(y)}{P(x_1)...P(x_n)}$\n",
    "\n",
    "y = $argmax_yP(y)\\sum_{i=1}^{n}P(x_i|y)$\n",
    "\n",
    "<b>Term Frequency - Inverse Document Frequency</b>\n",
    "\n",
    "Generally used to compute how important/relevant a word is to a certain document. It approaches 0 when it is a common word (e.g 'when', 'the', etc) and 1 when it is unique.\n",
    "\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, \n",
    "            y=train_labels_encoded);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Evaluate baseline on validation dataset\n",
    "model_0.score(X=val_sentences,\n",
    "              y=val_labels_encoded)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model Evaluation\n",
    "### ‚ùå Cross validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Make predictions using the Naive Bayes Classifier\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "\n",
    "# Make predictions using the K-fold cross validation\n",
    "y_val_preds = cross_val_predict(model_0, val_sentences, val_labels_encoded, cv=3)\n",
    "\n",
    "print(baseline_preds)\n",
    "print(y_val_preds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4 1 3 ... 4 4 1]\n",
      "[4 1 2 ... 4 4 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ü§î Confusion Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "'''\n",
    "An optimal confusion matrix would be one with non-zero integers in the diagonal (top left to bottom right)\n",
    "\n",
    "1 0 0 0 0\n",
    "0 1 0 0 0\n",
    "0 0 1 0 0\n",
    "0 0 0 1 0\n",
    "0 0 0 0 1\n",
    "'''\n",
    "\n",
    "confusion_matrix(val_labels_encoded, baseline_preds)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1683,  794,  694,   97,  181],\n",
       "       [ 244, 2687,  432,    5, 1214],\n",
       "       [ 116,   75, 8670,    7, 1096],\n",
       "       [ 505,  422,  978,  325,  146],\n",
       "       [  20,  181, 1196,    1, 8443]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    '''\n",
    "    Calculates model accuracy, precision, recall, f1 score of a binary classification model\n",
    "    \n",
    "    Args:\n",
    "        y_true: true labels in the form of a 1D array\n",
    "        y_pred: predicted labels in the form of a 1D array\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of metrics\n",
    "    '''\n",
    "    \n",
    "    # Calculate the model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    \n",
    "    # Calculate the model precision, recall and f1 score\n",
    "    model_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    model_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    model_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    model_results = {\n",
    "        \"accuracy\": model_accuracy,\n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall,\n",
    "        \"f1\": model_f1\n",
    "    }\n",
    "    return model_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ‚öñÔ∏è Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Calculate the baseline results on the validation dataset\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=y_val_preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "all_model_results = pd.DataFrame({\n",
    "    \"Multinomial Naive Bayes Classifier\": baseline_results,\n",
    "})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     accuracy        f1  precision    recall\n",
       "Multinomial Naive Bayes Classifier  66.483516  0.618607   0.668206  0.664835"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes Classifier</th>\n",
       "      <td>66.483516</td>\n",
       "      <td>0.618607</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.664835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above are the model results for the <b>baseline model</b>. We aim to beat these results in the upcoming experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "![title](img/model_1.png)\n",
    "\n",
    "# üë®üèΩ‚Äçüíª Dataset Analysis\n",
    "\n",
    "\n",
    "### Prepare data for deep sequence modelling\n",
    "In order to train our deep neural networks, we need to preprocess our data in a different way. Namely, tokenization and embedding layers. <br> \n",
    "The following code is to obtain more information about the `training_sentences` so we can create the tokenization layers with appropriate parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>‚úçüèΩAverage Sentence length<b>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Import relevant libraries for creating vectorization and embedding layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# If we want to create embedding layers, we need the sentences to be of equal lengths\n",
    "sentence_lengths = [len(sentence.split()) for sentence in train_sentences] # Must use .split() since we only want words\n",
    "avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "avg_sentence_length"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>üìäVisualize the distribution of sentence lengths<b>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Visualize the distribution of sentence lengths to get a clearer picture\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sentence_lengths, bins=7);"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like majority of the sentences are between 0 and 50 tokens in length"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# How long of a sentence covers 95% of the lengths?\n",
    "output_seq_len = int(np.percentile(sentence_lengths, 95))\n",
    "output_seq_len"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means that 95% of the sentences have 55 tokens or less. When creating the tokenization layer, we'll use this value to make all sentences the same length. Sentences with lengths below 55 get padded with zeros and those above get truncated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Calculate the maximum length of the sentences\n",
    "max(sentence_lengths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since 95% of sentences are below 55 tokens in length, it wouldn't make sense to set 296 as the maximum length because then, majority of the data we pass to our model would be zeros (from padding)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert sentences into numbers\n",
    "Method 1: <b> Text Vectorizer </b>\n",
    "<br>\n",
    "Method 2: <b> Tokenizer</b>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1Ô∏è‚É£ Creating a Text vectorizer\n",
    "Using the information from above, the appropriate value for `max_length` is 55."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Vocabulary size for the 20k dataset is 68000 according to the paper\n",
    "max_tokens = 68000\n",
    "max_length = 55"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Two methods, TextVectorization & Tokenizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens, # Number of words in vocabulary \n",
    "    output_sequence_length = max_length # Desired output of vectorized sentences\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Adapt textvectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-28 15:28:28.826303: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# Testing out the text vectorizer\n",
    "import random\n",
    "\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Actual Text:\\n{random_sentence}\\n\")\n",
    "print(f\"Length of text: {len(random_sentence.split())}\\n\")\n",
    "print(f\"Vectorized sentence:\\n{text_vectorizer([random_sentence])}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actual Text:\n",
      "brief fr is a simple , inexpensive technique that may reduce na in college health settings and help decrease delays in treatment seeking .\n",
      "\n",
      "Length of text: 24\n",
      "\n",
      "Vectorized sentence:\n",
      "[[ 960 4115   20    8  981 5432  475   28   91  270 3078    5 2114   97\n",
      "  1094    3  909  318 4791    5   19 3128    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Re-run the cell to get different sentences. Notice that shorter sentences get padded with more zeros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtain vocab list\n",
    "Obtain vocabulary of the dataset, using the `.get_vocabulary()` method of the text vectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "rct_20k_vocab = text_vectorizer.get_vocabulary()\n",
    "\n",
    "# Get the size of the vocabulary\n",
    "print(f\"Number of words in vocabulary: {len(rct_20k_vocab)}\\n\")\n",
    "print(f\"Most commmon words in vocabulary: {rct_20k_vocab[:5]}\\n\")\n",
    "print(f\"Least common words in vocabulary: {rct_20k_vocab[-5:]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words in vocabulary: 64841\n",
      "\n",
      "Most commmon words in vocabulary: ['', '[UNK]', 'the', 'and', 'of']\n",
      "\n",
      "Least common words in vocabulary: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the configuration of the text vectorizer using the `.get_config()` method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "text_vectorizer.get_config()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create text embeddings\n",
    "Earlier, we mapped words to numbers, but this doesn't capture relationships between the numbers. To do this, we can use <b>embeddings</b>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "# Create token embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_vocab), # length of vocabulary\n",
    "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
    "                               # Use masking to handle variable sequence lengths (save space)\n",
    "                               mask_zero=True,\n",
    "                               name=\"token_embedding\") \n",
    "\n",
    "# Show an instance of embedding\n",
    "print(f\"Sentence before embedding:\\n{random_sentence}\\n\")\n",
    "\n",
    "vectorized_sentence = text_vectorizer([random_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
    "\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
    "print(f\"Shape of embedded sentence: {embedded_sentence.shape}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence before embedding:\n",
      "brief fr is a simple , inexpensive technique that may reduce na in college health settings and help decrease delays in treatment seeking .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[[ 960 4115   20    8  981 5432  475   28   91  270 3078    5 2114   97\n",
      "  1094    3  909  318 4791    5   19 3128    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[-0.01750016 -0.01254452 -0.04888484 ...  0.02556861 -0.00914514\n",
      "   -0.04283677]\n",
      "  [ 0.02729143  0.0266268  -0.03680147 ... -0.01211009 -0.00781734\n",
      "    0.02262756]\n",
      "  [ 0.03698936 -0.0203568   0.03268761 ...  0.04339674  0.04158126\n",
      "   -0.01645691]\n",
      "  ...\n",
      "  [-0.01257067 -0.02364277  0.02363184 ...  0.01493958  0.02573258\n",
      "   -0.00991319]\n",
      "  [-0.01257067 -0.02364277  0.02363184 ...  0.01493958  0.02573258\n",
      "   -0.00991319]\n",
      "  [-0.01257067 -0.02364277  0.02363184 ...  0.01493958  0.02573258\n",
      "   -0.00991319]]]\n",
      "\n",
      "Shape of embedded sentence: (1, 55, 128)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create datasets (for quick runtime)\n",
    "The tf.data API provides methods that enable faster data loading <br>\n",
    "Create a PrefetchDataset of batches from our data that we have prepared. Use the batch() and prefetch() method, and the tf.data.AUTOTUNE (allow tf to determine the optimal amount of compute to prepare datasets)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Model Training\n",
    "\n",
    "The input data to a deep sequence model is different from the one we feeded into a naive bayes model. We need to prepare data in another format\n",
    "> Input (text) ‚û° Tokenizer ‚û° Embedding ‚û° Layers ‚û° Output (label probability)\n",
    "\n",
    "After building our model\n",
    "> Build model ‚û° Train model ‚û° Evaluate model (compare with ground truth and baseline model performance)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Create 1D convolutional model to process sequences\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = token_embed(text_vectors) # create embedding\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "model_1.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "token_embedding (Embedding)  (None, 55, 128)           8299648   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 55, 64)            41024     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "                              epochs=3,\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.9223 - accuracy: 0.6299 - val_loss: 0.7014 - val_accuracy: 0.7317\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.6688 - accuracy: 0.7512 - val_loss: 0.6407 - val_accuracy: 0.7703\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.6247 - accuracy: 0.7713 - val_loss: 0.5994 - val_accuracy: 0.7862\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model Evaluation\n",
    "### ‚ùå Cross validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "model_1.evaluate(valid_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "945/945 [==============================] - 1s 1ms/step - loss: 0.6028 - accuracy: 0.7853\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6028474569320679, 0.785317063331604]"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Make predictions (our model outputs prediction probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4.1601500e-01, 1.8071696e-01, 7.6475225e-02, 2.9905215e-01,\n",
       "        2.7740702e-02],\n",
       "       [4.1035190e-01, 3.2293937e-01, 8.0038356e-03, 2.5223228e-01,\n",
       "        6.4725862e-03],\n",
       "       [1.5408161e-01, 1.2729758e-02, 1.8278150e-03, 8.3132201e-01,\n",
       "        3.8706505e-05],\n",
       "       ...,\n",
       "       [2.6711325e-06, 5.7343312e-04, 8.0793485e-04, 1.8414834e-06,\n",
       "        9.9861407e-01],\n",
       "       [5.6430999e-02, 5.3369349e-01, 7.5182706e-02, 6.4857133e-02,\n",
       "        2.6983565e-01],\n",
       "       [2.2802907e-01, 5.5391818e-01, 5.8656145e-02, 8.1564553e-02,\n",
       "        7.7832095e-02]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# Convert these probabilities into classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])>"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ‚öñÔ∏è Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_1_preds)\n",
    "model_1_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 78.53170925460083,\n",
       " 'precision': 0.7819417654925646,\n",
       " 'recall': 0.7853170925460082,\n",
       " 'f1': 0.7828465371798523}"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "all_model_results = pd.DataFrame({\n",
    "    \"Multinomial Naive Bayes Classifier\": baseline_results,\n",
    "    \"Conv1D w/ token embeddings\": model_1_results,\n",
    "})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     accuracy  precision    recall        f1\n",
       "Multinomial Naive Bayes Classifier  66.483516   0.668206  0.664835  0.618607\n",
       "Conv1D w/ token embeddings          78.531709   0.781942  0.785317  0.782847"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes Classifier</th>\n",
       "      <td>66.483516</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.618607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conv1D w/ token embeddings</th>\n",
       "      <td>78.531709</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.785317</td>\n",
       "      <td>0.782847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "model_1.save('model_1/saved_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-28 15:30:30.046882: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_1/saved_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "![title](img/model_2.png) \n",
    "\n",
    "\n",
    "Use the pretrained embeddings as a way to initialize token embeddings\n",
    "\n",
    "The tokenizer layer is _missing_, but the Universal Sentence Encoder will take care of the tokenisation for us.\n",
    "\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Model Training\n",
    "\n",
    "### Use a pretrained embedding layer from TFHub\n",
    "To download the pretrained USE, and use it in our model, we can use `Hub.KerasLayer`.\n",
    "Keep the pre-trained embedding layer(by setting the method `trainable=False` and add a trainable couple of layers on the top to tailor the model outputs to our own data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# the model is relatively large, so it may take a while to load\n",
    "universal_sentence_encoder = hub.KerasLayer(\"./data/universal-sentence-encoder_4\",\n",
    "                                       trainable=False,\n",
    "                                       name=\"universal_sentence_encoder\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# Test the layer\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Random training sentence:\\n{random_sentence}\\n\")\n",
    "\n",
    "embedded_sentence = universal_sentence_encoder([random_sentence])\n",
    "print(f\"Sentence after embeddding:\\n{embedded_sentence}\\n\")\n",
    "\n",
    "print(f\"Length of sentence after embeddding:\\n{len(embedded_sentence[0])}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random training sentence:\n",
      "maternal depressive symptoms moderated the effect of cbt , but not sbft , on adolescents ' problem solving ; adolescents experienced increases in problem solving only when their mothers had low or moderate levels of depressive symptoms .\n",
      "\n",
      "Sentence after embeddding:\n",
      "[[ 3.93456742e-02 -1.14758476e-03  3.34811024e-02 -4.88323942e-02\n",
      "  -7.34819919e-02  6.60400689e-02 -3.67805897e-03 -4.38062325e-02\n",
      "   1.40553871e-02  5.85198291e-02  7.36998990e-02  1.10934430e-03\n",
      "   4.86762822e-02  1.00040762e-02  1.56840552e-02 -7.54158720e-02\n",
      "  -7.55787566e-02  6.53630123e-02  1.60627868e-02 -7.35875815e-02\n",
      "   5.53362668e-02  7.14916959e-02 -3.09081376e-02 -4.94390912e-02\n",
      "   5.18502519e-02 -3.14867236e-02 -2.69715711e-02 -3.93605381e-02\n",
      "   1.44846337e-02  1.07851718e-02 -3.94855849e-02  7.49554783e-02\n",
      "  -1.49984844e-02  3.82143855e-02 -3.55118476e-02  6.33953139e-02\n",
      "   1.15757845e-02 -5.69755323e-02 -2.63111386e-02 -5.60043454e-02\n",
      "   5.93202673e-02  7.47235343e-02 -4.57361154e-03 -3.49451266e-02\n",
      "   2.15849634e-02  2.22907197e-02  5.42636178e-02  4.95726280e-02\n",
      "  -6.92082793e-02  2.96750721e-02 -1.42228194e-02  1.95081672e-03\n",
      "   1.12505862e-02 -5.29421493e-02  6.26915768e-02  9.63659678e-03\n",
      "  -6.28044680e-02 -4.20939103e-02  5.32404296e-02 -6.74119638e-03\n",
      "  -5.86027503e-02 -3.33355255e-02 -4.98328581e-02  1.54275820e-02\n",
      "  -5.39036617e-02 -3.55315246e-02  6.69882894e-02 -8.10291432e-03\n",
      "   1.46101089e-02  4.25877385e-02  4.79981676e-02 -1.89834200e-02\n",
      "  -1.15815885e-02  4.51534875e-02 -7.09744543e-02  7.40724877e-02\n",
      "  -5.33865690e-02  5.40984347e-02  7.32387900e-02  6.44938126e-02\n",
      "   2.28515584e-02  6.17890842e-02 -1.77092124e-02 -2.89116334e-02\n",
      "   5.94940335e-02  5.23147173e-02 -2.61486508e-03  6.24575801e-02\n",
      "   3.33408825e-02 -6.63456246e-02  4.58749831e-02  2.64208037e-02\n",
      "  -4.46715467e-02 -6.24906458e-02  6.14040084e-02  6.32802844e-02\n",
      "   3.27116624e-02 -9.07680485e-03  3.69301252e-02 -5.91814741e-02\n",
      "   3.87225859e-02  5.03808185e-02 -5.77904843e-02  1.29604698e-04\n",
      "   4.30691801e-02  1.36578726e-02 -4.99367379e-02  5.55320047e-02\n",
      "   2.38453131e-02 -2.86270697e-02 -3.73412855e-02 -1.78810991e-02\n",
      "   6.79813847e-02 -7.31148943e-02  4.43828553e-02 -2.85995472e-02\n",
      "   2.09683459e-02 -4.20074072e-03  6.91927820e-02  1.69156434e-03\n",
      "   1.10722454e-02  6.37532026e-02 -2.72653475e-02  4.85645607e-02\n",
      "   6.44624010e-02 -3.52017172e-02 -5.84003180e-02 -7.36071244e-02\n",
      "   6.51788414e-02  2.74187494e-02  2.07971316e-02  7.35458732e-02\n",
      "   3.84305567e-02  3.29141715e-03  5.96260838e-02 -6.87733218e-02\n",
      "  -3.18569653e-02 -5.98481018e-03  4.15733494e-02 -2.33703200e-02\n",
      "   6.30345419e-02  6.63771629e-02 -1.88903082e-02  2.43816506e-02\n",
      "  -3.23720649e-03 -3.02547477e-02  1.42423809e-02  7.55234361e-02\n",
      "  -4.25059423e-02 -3.41420472e-02  4.88114990e-02  3.61835547e-02\n",
      "  -4.84474786e-02  2.01440919e-02 -3.16436291e-02 -3.85029465e-02\n",
      "  -3.09568755e-02 -1.44891785e-02 -5.18720597e-02 -6.53612688e-02\n",
      "   1.02578644e-02  1.40061090e-02 -6.41307514e-03 -6.17244542e-02\n",
      "   1.73601620e-02  4.10812311e-02 -8.82329512e-03 -3.71873267e-02\n",
      "   1.12407859e-02 -7.80671882e-03 -4.47595045e-02 -7.55762979e-02\n",
      "  -6.61162063e-02 -6.40090462e-03 -6.63796738e-02  4.35586646e-02\n",
      "  -6.98008621e-03  7.03716092e-03  2.93224417e-02  6.34883717e-02\n",
      "   3.38220932e-02 -4.27619629e-02 -3.41980010e-02  9.16278083e-03\n",
      "   6.96837157e-02  3.79599482e-02  8.26814398e-03 -6.36871979e-02\n",
      "  -5.18195964e-02 -8.99432041e-03  6.50674254e-02 -3.98668088e-02\n",
      "   5.50204962e-02  6.75979108e-02  3.60847563e-02  2.48655342e-02\n",
      "   5.10960594e-02  4.55630533e-02  7.79045047e-03 -4.88695353e-02\n",
      "  -2.28851195e-02  3.03817857e-02 -4.47871275e-02 -5.55901118e-02\n",
      "   6.28277063e-02  4.55359258e-02  5.05432906e-03 -8.75410624e-04\n",
      "  -2.10328531e-02  7.02054650e-02 -7.94266700e-04  9.33916867e-03\n",
      "  -1.24770040e-02  3.10966540e-02 -6.67303652e-02 -6.32929876e-02\n",
      "   5.94872721e-02  3.71862277e-02 -2.09174715e-02 -3.44404243e-02\n",
      "  -6.13078754e-03  5.19621186e-02 -5.41480482e-02  4.85978164e-02\n",
      "   5.46789169e-03  6.74497187e-02  4.07951921e-02 -6.29874542e-02\n",
      "   3.06005310e-02  5.42849936e-02 -1.00023393e-03  4.39479016e-02\n",
      "  -7.08841532e-02 -6.80460781e-02 -6.15503965e-03 -6.31949082e-02\n",
      "   1.84084103e-02 -5.72749339e-02 -4.07012142e-02  3.47052887e-03\n",
      "   7.46300630e-03  6.55104592e-02  2.21268665e-02 -1.82446372e-02\n",
      "  -4.44348454e-02 -4.61789928e-02 -6.32418096e-02 -4.99716923e-02\n",
      "   4.63661663e-02 -7.33120814e-02  1.62754226e-02 -6.37525097e-02\n",
      "  -1.79084297e-02 -3.17253582e-02  6.47209957e-02  2.16363575e-02\n",
      "  -2.46569011e-02  6.78384006e-02 -2.62189917e-02 -3.73067930e-02\n",
      "  -1.27464626e-02  2.57060733e-02  1.70008782e-02  6.86871111e-02\n",
      "  -6.73907176e-02  1.06917731e-02 -5.88071235e-02 -5.99622168e-03\n",
      "   1.17559489e-02 -3.06902621e-02 -6.06837422e-02 -1.80372912e-02\n",
      "   4.81421314e-02 -6.73802942e-02  5.63126523e-03  1.72013212e-02\n",
      "  -5.43174520e-02  1.64095536e-02  3.99402939e-02 -4.06246670e-02\n",
      "   5.24679373e-04 -2.35895487e-03  1.57203507e-02  1.77763533e-02\n",
      "  -6.92593604e-02  1.13029042e-02  7.55471140e-02  3.79301496e-02\n",
      "   4.44177352e-02  5.36363833e-02 -6.90711588e-02  5.87342195e-02\n",
      "   3.74624990e-02 -2.62488630e-02 -5.93231805e-03 -4.17745747e-02\n",
      "  -7.38579128e-03  9.15749837e-03 -6.34045899e-02  6.58684000e-02\n",
      "  -1.82101633e-02  3.62587757e-02 -4.17428697e-03 -5.31207696e-02\n",
      "   3.53063531e-02  1.67725086e-02  7.00917765e-02 -1.66817494e-02\n",
      "   5.21602184e-02  4.14647162e-02  5.31044882e-03 -5.08140698e-02\n",
      "   1.69459041e-02  6.84126243e-02  1.39838653e-02  5.95292747e-02\n",
      "  -2.36001629e-02  2.57544890e-02  7.24514797e-02 -6.14721030e-02\n",
      "  -5.66821098e-02 -2.68267933e-02  5.10527827e-02 -2.95708179e-02\n",
      "  -5.98852038e-02  2.58731879e-02 -1.30892573e-02 -3.32698897e-02\n",
      "  -5.06199561e-02  9.56788845e-03 -1.26766749e-02  1.76747832e-02\n",
      "  -2.92165689e-02  6.87747076e-02 -7.27478191e-02 -5.42674623e-02\n",
      "  -4.63839695e-02  6.32607564e-02 -5.69837354e-02 -4.53364551e-02\n",
      "  -4.05743383e-02 -3.66245098e-02 -2.69705299e-02  2.92526111e-02\n",
      "  -5.74855059e-02 -2.60353144e-02  5.83957694e-02 -6.83743954e-02\n",
      "  -3.06023248e-02 -5.33736162e-02  2.75610704e-02  4.48536575e-02\n",
      "   2.63824426e-02 -3.37120192e-03  6.38806298e-02  5.58113120e-03\n",
      "  -7.26665035e-02  3.93144116e-02  4.96177450e-02 -5.91151305e-02\n",
      "   2.33511976e-03  4.82665859e-02 -3.28406808e-03 -4.81103510e-02\n",
      "   2.53794882e-02  1.12208677e-02  3.18584964e-02  4.51533943e-02\n",
      "   3.82365100e-02 -2.31146310e-02 -4.43676636e-02  2.36010831e-02\n",
      "  -1.99257582e-02  7.11092502e-02 -1.62401255e-02  1.32649876e-02\n",
      "  -3.53485644e-02  4.35669459e-02 -2.90066302e-02 -4.95197363e-02\n",
      "   5.56674935e-02 -4.34480496e-02  7.01203421e-02  6.23221286e-02\n",
      "   5.09768538e-02 -2.70262621e-02  5.04140332e-02 -4.03403454e-02\n",
      "  -3.38116512e-02  1.71114486e-02 -1.12675305e-03  3.89155336e-02\n",
      "  -7.16050118e-02  2.26299139e-03  4.03603576e-02 -2.32374407e-02\n",
      "   4.14422005e-02  2.41871253e-02  3.09524406e-02 -6.44502342e-02\n",
      "   4.91373204e-02 -7.22988844e-02 -6.45410195e-02  5.06266057e-02\n",
      "  -6.92816302e-02 -2.87922402e-03 -5.59171848e-02 -5.68329059e-02\n",
      "   1.07687144e-02  4.52979244e-02 -1.00473231e-02  4.27760929e-03\n",
      "   5.28698899e-02 -2.35212278e-02  3.48689407e-02  6.70931116e-02\n",
      "  -4.32383344e-02  2.77442131e-02  1.52322361e-02 -5.72230406e-02\n",
      "   4.63514961e-02  2.57147457e-02  2.73926184e-02 -9.73338168e-03\n",
      "   4.35293205e-02 -2.53314283e-02  5.80838770e-02  1.65555160e-02\n",
      "  -5.28101213e-02  1.35110896e-02 -5.88406697e-02 -6.36402741e-02\n",
      "  -6.81255832e-02  2.65839323e-02 -5.22852764e-02 -2.79948711e-02\n",
      "   3.09617468e-03 -3.10896896e-02  6.46558702e-02  7.21569806e-02\n",
      "  -2.18657474e-03  5.39394841e-02  4.48246375e-02  4.76511903e-02\n",
      "   1.81800853e-02 -2.96101086e-02 -5.37025705e-02 -5.80885746e-02\n",
      "   5.75075597e-02 -5.37871756e-02 -2.78416798e-02 -7.36711081e-04\n",
      "   8.36254749e-03  2.51965858e-02  1.61225423e-02 -6.74811676e-02\n",
      "   5.08242287e-02  5.74093685e-02 -4.61154012e-03  6.90450519e-02\n",
      "  -7.53446519e-02  3.20286676e-02  6.12589084e-02 -1.29239857e-02\n",
      "   6.53422475e-02  6.32257313e-02 -2.68008504e-02  6.78755823e-05\n",
      "   6.83594681e-03  5.45946173e-02  3.43296379e-02  3.31147760e-02\n",
      "  -6.50372878e-02 -2.63781915e-03 -1.63759440e-02  3.44446255e-03\n",
      "   5.55116162e-02  4.62978370e-02  7.31840804e-02  3.43915075e-02\n",
      "   1.55969830e-02  6.94048181e-02 -1.57418009e-02 -7.24295229e-02\n",
      "   4.21750620e-02  5.51884323e-02  2.26365793e-02 -5.18383905e-02\n",
      "   1.54864946e-02 -4.42400388e-02 -3.74469906e-02 -6.19577207e-02\n",
      "   6.20153137e-02  1.85992513e-02  1.23677347e-02 -2.47922800e-02\n",
      "   3.26798782e-02 -2.49020546e-03  2.61151399e-02 -6.49128258e-02\n",
      "  -6.24315850e-02 -3.68188992e-02  9.43373516e-03 -3.44280191e-02\n",
      "   2.86174193e-03 -7.55213127e-02  6.15029456e-03 -2.44939718e-02\n",
      "   3.73652875e-02 -4.30840403e-02 -6.00931011e-02  2.72823106e-02]]\n",
      "\n",
      "Length of sentence after embeddding:\n",
      "512\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# Build a model around the tf_hub_embedding_layer\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "# Tokenize and embedding layer\n",
    "pretrained_embedding_layer = universal_sentence_encoder(inputs)\n",
    "\n",
    "# Dense layer\n",
    "x = layers.Dense(128, activation='relu')(pretrained_embedding_layer)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model_2 = tf.keras.Model(inputs=inputs,\n",
    "                        outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the model summary\n",
    "model_2.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None,)]                 0         \n",
      "_________________________________________________________________\n",
      "universal_sentence_encoder ( (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 256,864,133\n",
      "Trainable params: 66,309\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the summary, most of the parameters are non-trainable, this is because we set `trainable=False` when initializing the Universal Sentence Encoder (USE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "if os.path.exists('model_2/saved_model'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_2 = tf.keras.models.load_model('model_2/saved_model')\n",
    "    \n",
    "else:\n",
    "    model_2_history = model_2.fit(train_dataset,\n",
    "                                 steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                                 epochs=3,\n",
    "                                 validation_data=valid_dataset,\n",
    "                                 validation_steps=int(0.1 * len(valid_dataset)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 6s 7ms/step - loss: 0.9157 - accuracy: 0.6487 - val_loss: 0.7960 - val_accuracy: 0.6882\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 4s 7ms/step - loss: 0.7673 - accuracy: 0.7022 - val_loss: 0.7523 - val_accuracy: 0.7068\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 4s 7ms/step - loss: 0.7502 - accuracy: 0.7135 - val_loss: 0.7365 - val_accuracy: 0.7131\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "model_2.evaluate(valid_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "945/945 [==============================] - 5s 5ms/step - loss: 0.7386 - accuracy: 0.7145\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.738615095615387, 0.7144511938095093]"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# MAKE PREDICTIONS WITH THE NEWLY TRAINED MODEL\n",
    "\n",
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "model_2_pred_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4.3255129e-01, 3.5521135e-01, 2.0533316e-03, 2.0135774e-01,\n",
       "        8.8263052e-03],\n",
       "       [3.4356654e-01, 4.8924577e-01, 3.0465256e-03, 1.6146268e-01,\n",
       "        2.6784164e-03],\n",
       "       [2.4661611e-01, 1.4951919e-01, 1.9181486e-02, 5.4299027e-01,\n",
       "        4.1692950e-02],\n",
       "       ...,\n",
       "       [2.1114282e-03, 5.5661756e-03, 5.6280199e-02, 8.0316979e-04,\n",
       "        9.3523908e-01],\n",
       "       [3.8391377e-03, 5.0732717e-02, 2.0159803e-01, 1.2064138e-03,\n",
       "        7.4262375e-01],\n",
       "       [1.7332198e-01, 2.7035755e-01, 5.0216436e-01, 5.5560223e-03,\n",
       "        4.8600171e-02]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# Convert the predictions into labels\n",
    "\n",
    "# Use axis=1 to get argmax of lists in the second axis. If axis=0 is used, indexes of the list containing the largest value for each axis will be returned\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1) # axis=0 by default\n",
    "model_2_preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 4, 2])>"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "model_2_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_2_preds)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare the model results so far"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "all_model_results = pd.DataFrame({\n",
    "    \"baseline\": baseline_results,\n",
    "    \"custom_token_embedding\": model_1_results,\n",
    "    \"pretrained token embedding layer\": model_2_results\n",
    "})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   accuracy  precision    recall        f1\n",
       "baseline                          66.483516   0.668206  0.664835  0.618607\n",
       "custom_token_embedding            78.531709   0.781942  0.785317  0.782847\n",
       "pretrained token embedding layer  71.445121   0.714712  0.714451  0.711513"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>66.483516</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.618607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_token_embedding</th>\n",
       "      <td>78.531709</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.785317</td>\n",
       "      <td>0.782847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained token embedding layer</th>\n",
       "      <td>71.445121</td>\n",
       "      <td>0.714712</td>\n",
       "      <td>0.714451</td>\n",
       "      <td>0.711513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "model_2.save('model_2/saved_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_2/saved_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "![title](img/model_3.png) \n",
    "\n",
    "\n",
    "# üë®üèΩ‚Äçüíª Dataset Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# Define a helper function to split the sentences into characters\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "# Test out the function\n",
    "split_chars(random_sentence)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"m a t e r n a l   d e p r e s s i v e   s y m p t o m s   m o d e r a t e d   t h e   e f f e c t   o f   c b t   ,   b u t   n o t   s b f t   ,   o n   a d o l e s c e n t s   '   p r o b l e m   s o l v i n g   ;   a d o l e s c e n t s   e x p e r i e n c e d   i n c r e a s e s   i n   p r o b l e m   s o l v i n g   o n l y   w h e n   t h e i r   m o t h e r s   h a d   l o w   o r   m o d e r a t e   l e v e l s   o f   d e p r e s s i v e   s y m p t o m s   .\""
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Split sentence into character-level split data \n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "print(train_chars[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .\n"
     ]
    }
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# What's the average character length\n",
    "char_lengths = [len(sentence) for sentence in train_sentences] # Unlike the token-level split data, don't use the .split() method as we want number of characters\n",
    "mean_char_length = np.mean(char_lengths)\n",
    "\n",
    "mean_char_length"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# Check the distribution of our sequences at character-level\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(char_lengths, bins=7);"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like majority of the sequences are between 0 and 200 characters long.\n",
    "\n",
    "Find out the 95th percentile of sequence lengths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "output_seq_char_length = int(np.percentile(char_lengths,95))\n",
    "output_seq_char_length"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the optimal output sequence length, we need to find the value of max_tokens, and other parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Get all the keyboard characters for character level embedding\n",
    "import string\n",
    "\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# Create a character level token vectorizer instance\n",
    "max_tokens = len(alphabet)\n",
    "\n",
    "char_vectorizer = TextVectorization(max_tokens=max_tokens,\n",
    "                                   output_sequence_length=output_seq_char_length,\n",
    "                                   standardize='lower_and_strip_punctuation',\n",
    "                                   name='char_vectorizer')\n",
    "\n",
    "# Adapt the training data to the char-level vectorizer\n",
    "char_vectorizer.adapt(train_chars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After adapting the vectorizer to the char-level vectorizer, let's check some charateristics about it and obtain the vocabulary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "\n",
    "print(f\"Number of unique characters in the vocabulary:\\n{len(char_vocab)}\\n\")\n",
    "print(f\"Least commmon characters:\\n{char_vocab[-5:]}\\n\")\n",
    "print(f\"Most common characters:\\n{char_vocab[:5]}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of unique characters in the vocabulary:\n",
      "28\n",
      "\n",
      "Least commmon characters:\n",
      "['k', 'x', 'z', 'q', 'j']\n",
      "\n",
      "Most common characters:\n",
      "['', '[UNK]', 'e', 't', 'i']\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test it on random sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "random_train_chars = random.choice(train_chars)\n",
    "\n",
    "print(f\"Text that is split at character-level:\\n{random_train_chars}\\n\")\n",
    "print(f\"Length of character sequence:\\n{len(random_train_chars.split())}\\n\")\n",
    "\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"Vectorized character sequence:\\n{vectorized_chars}\\n\")\n",
    "print(f\"Length of vectorized character sequences(after padding):\\n{len(vectorized_chars[0])}\") # Access the zeroth index because of the double nested list"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text that is split at character-level:\n",
      "h o w e v e r   ,   s y s t e m a t i c   f a c t o r s   i n c l u d i n g   t h e   s h o r t a g e   o f   t r a i n e d   p r o f e s s i o n a l s   a n d   t h e   r e l a t i v e   u n d e r d e v e l o p m e n t   o f   s e r v i c e s   a l s o   m a k e   a c c e s s   d i f f i c u l t . s t e p p e d - c a r e   c a n   i n c r e a s e   a c c e s s   t o   e v i d e n c e - b a s e d   c b t   .\n",
      "\n",
      "Length of character sequence:\n",
      "180\n",
      "\n",
      "Vectorized character sequence:\n",
      "[[13  7 20  2 21  2  8  9 19  9  3  2 15  5  3  4 11 17  5 11  3  7  8  9\n",
      "   4  6 11 12 16 10  4  6 18  3 13  2  9 13  7  8  3  5 18  2  7 17  3  8\n",
      "   5  4  6  2 10 14  8  7 17  2  9  9  4  7  6  5 12  9  5  6 10  3 13  2\n",
      "   8  2 12  5  3  4 21  2 16  6 10  2  8 10  2 21  2 12  7 14 15  2  6  3\n",
      "   7 17  9  2  8 21  4 11  2  9  5 12  9  7 15  5 23  2  5 11 11  2  9  9\n",
      "  10  4 17 17  4 11 16 12  3  9  3  2 14 14  2 10 11  5  8  2 11  5  6  4\n",
      "   6 11  8  2  5  9  2  5 11 11  2  9  9  3  7  2 21  4 10  2  6 11  2 22\n",
      "   5  9  2 10 11 22  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "\n",
      "Length of vectorized character sequences(after padding):\n",
      "290\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequences that are shorter than 290 characters in length are <b>padded</b> while those that are longer get truncated, ensuring that all sequences passed into the model are equal in length. \n",
    "\n",
    "The `standardize` parameter being set to `lower_and_strip_punctuation` and the split function ensures that punctuation, symbols and whitespaces are removed during the vectorization process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create character-level embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "char_embed = layers.Embedding(input_dim=max_tokens,\n",
    "                             output_dim=25,\n",
    "                             mask_zero=True,\n",
    "                             name='char_embed')\n",
    "\n",
    "# Test out the character embedding layer\n",
    "print(f\"Character-level text(before embedding and vectorization):\\n{random_train_chars}\\n\")\n",
    "\n",
    "random_char_embed = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded Characters (after vectorization and embedding):\\n{random_char_embed}\\n\")\n",
    "print(f\"Character-embedded shape: {random_char_embed.shape}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Character-level text(before embedding and vectorization):\n",
      "h o w e v e r   ,   s y s t e m a t i c   f a c t o r s   i n c l u d i n g   t h e   s h o r t a g e   o f   t r a i n e d   p r o f e s s i o n a l s   a n d   t h e   r e l a t i v e   u n d e r d e v e l o p m e n t   o f   s e r v i c e s   a l s o   m a k e   a c c e s s   d i f f i c u l t . s t e p p e d - c a r e   c a n   i n c r e a s e   a c c e s s   t o   e v i d e n c e - b a s e d   c b t   .\n",
      "\n",
      "Embedded Characters (after vectorization and embedding):\n",
      "[[[ 0.02781377  0.04172577 -0.02508597 ... -0.03270992 -0.00969479\n",
      "   -0.04536424]\n",
      "  [-0.02203144 -0.02512615 -0.02599045 ... -0.01138477  0.03838642\n",
      "   -0.0101267 ]\n",
      "  [-0.04836696  0.04893669  0.03181137 ...  0.04050303 -0.03547303\n",
      "    0.04371593]\n",
      "  ...\n",
      "  [ 0.01268393  0.04179719 -0.03621278 ...  0.02073069 -0.01119449\n",
      "    0.03940651]\n",
      "  [ 0.01268393  0.04179719 -0.03621278 ...  0.02073069 -0.01119449\n",
      "    0.03940651]\n",
      "  [ 0.01268393  0.04179719 -0.03621278 ...  0.02073069 -0.01119449\n",
      "    0.03940651]]]\n",
      "\n",
      "Character-embedded shape: (1, 290, 25)\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basically, for one sentence, there are 290 characters, and each character has 25 dimensions, as stated in the `output_dim` method.\n",
    "\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Make Conv1D for characters only\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding='same', activation='relu')(char_embeddings)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        name='model_3_conv1d_char_embedding')\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Check the summary of the char_embedding model\n",
    "model_3.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_3_conv1d_char_embedding\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "char_vectorizer (TextVectori (None, 290)               0         \n",
      "_________________________________________________________________\n",
      "char_embed (Embedding)       (None, 290, 25)           1700      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 290, 64)           8064      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 10,089\n",
      "Trainable params: 10,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Char Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                             steps_per_epoch = int(0.1 * len(train_char_dataset)),\n",
    "                             epochs=3,\n",
    "                             validation_data = val_char_dataset,\n",
    "                             validation_steps = int(0.1*len(val_char_dataset)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 5s 8ms/step - loss: 1.2595 - accuracy: 0.4820 - val_loss: 1.0416 - val_accuracy: 0.5901\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 5s 8ms/step - loss: 1.0088 - accuracy: 0.5944 - val_loss: 0.9415 - val_accuracy: 0.6376\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 5s 8ms/step - loss: 0.9265 - accuracy: 0.6350 - val_loss: 0.8649 - val_accuracy: 0.6672\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "model_3.evaluate(val_char_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "945/945 [==============================] - 1s 2ms/step - loss: 0.8894 - accuracy: 0.6550\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.8894333839416504, 0.6550377607345581]"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_pred_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.12289768, 0.32492566, 0.06808171, 0.44896588, 0.03512901],\n",
       "       [0.19527265, 0.49015617, 0.01642498, 0.2866022 , 0.01154403],\n",
       "       [0.12380609, 0.13964297, 0.18151167, 0.5292199 , 0.02581931],\n",
       "       ...,\n",
       "       [0.0182239 , 0.03506847, 0.2732532 , 0.03597685, 0.6374776 ],\n",
       "       [0.01678482, 0.07080121, 0.40693   , 0.05379524, 0.45168874],\n",
       "       [0.45472994, 0.3646986 , 0.08789218, 0.07548873, 0.01719058]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([3, 1, 3, ..., 4, 4, 0])>"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "model_3_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_3_preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ‚öñÔ∏è Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# Results of the models built so far\n",
    "all_model_results = pd.DataFrame({\n",
    "    \"baseline\": baseline_results,\n",
    "    \"custom_token_embedding\": model_1_results,\n",
    "    \"pretrained token embedding layer\": model_2_results,\n",
    "    \"Conv1D with character-level embedding\":model_3_results\n",
    "})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        accuracy  precision    recall  \\\n",
       "baseline                               66.483516   0.668206  0.664835   \n",
       "custom_token_embedding                 78.531709   0.781942  0.785317   \n",
       "pretrained token embedding layer       71.445121   0.714712  0.714451   \n",
       "Conv1D with character-level embedding  65.503773   0.650150  0.655038   \n",
       "\n",
       "                                             f1  \n",
       "baseline                               0.618607  \n",
       "custom_token_embedding                 0.782847  \n",
       "pretrained token embedding layer       0.711513  \n",
       "Conv1D with character-level embedding  0.645507  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>66.483516</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.618607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_token_embedding</th>\n",
       "      <td>78.531709</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.785317</td>\n",
       "      <td>0.782847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained token embedding layer</th>\n",
       "      <td>71.445121</td>\n",
       "      <td>0.714712</td>\n",
       "      <td>0.714451</td>\n",
       "      <td>0.711513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conv1D with character-level embedding</th>\n",
       "      <td>65.503773</td>\n",
       "      <td>0.650150</td>\n",
       "      <td>0.655038</td>\n",
       "      <td>0.645507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "model_3.save('model_3/saved_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_3/saved_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "![title](img/model_4.png) \n",
    "\n",
    "The next experiment is a hybrid model that uses both word and character embeddings, as mentioned in the paper.\n",
    "\n",
    "### üöÇ Build a input pipeline using the tf.data API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "# Combine the char and token into one dataset\n",
    "train_hybrid_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars))\n",
    "train_hybrid_labels = tf.data.Dataset.from_tensor_slices((train_labels_one_hot)) # Both have same labels anyway\n",
    "train_hybrid_dataset = tf.data.Dataset.zip((train_hybrid_data, train_hybrid_labels))\n",
    "\n",
    "# Prefetch and batch training data\n",
    "train_hybrid_dataset = train_hybrid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Repeat for validation data\n",
    "val_hybrid_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
    "val_hybrid_labels = tf.data.Dataset.from_tensor_slices((val_labels_one_hot)) # Both have same labels anyway\n",
    "val_hybrid_dataset = tf.data.Dataset.zip((val_hybrid_data, val_hybrid_labels))\n",
    "val_hybrid_dataset = val_hybrid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "train_hybrid_dataset, val_hybrid_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)>,\n",
       " <PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)>)"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# Build the token-level embedding layer (similar to model_1)\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name='token_input')\n",
    "token_embeddings = universal_sentence_encoder(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation='relu')(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                            outputs=token_outputs)\n",
    "\n",
    "# Build the character-level embedding layer (similar to model_3 with a slight modification to reflect the paper)\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name='char_input')\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                           outputs=char_bi_lstm)\n",
    "\n",
    "# Combine the two outputs from the models 1 and 2, using layers.Concatenate\n",
    "token_char_concat = layers.Concatenate(name='token_char_hybrid')([token_model.output, char_model.output])\n",
    "\n",
    "# Final output layers similar to the ones in the paper\n",
    "x = layers.Dropout(0.5)(token_char_concat)\n",
    "x = layers.Dense(200, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output_layers = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Construct a model that take character-level and token-level sequences as input, and outputs label probabilities.\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "                        outputs=output_layers,\n",
    "                        name='model_4_hybrid_token_and_char_embeddings')\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "model_4.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_4_hybrid_token_and_char_embeddings\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_vectorizer (TextVectorizat (None, 290)          0           char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "universal_sentence_encoder (Ker (None, 512)          256797824   token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, 290, 25)      1700        char_vectorizer[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          65664       universal_sentence_encoder[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50)           10200       char_embed[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "token_char_hybrid (Concatenate) (None, 178)          0           dense_4[0][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 178)          0           token_char_hybrid[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 200)          35800       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            1005        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 256,912,193\n",
      "Trainable params: 114,369\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "model_4_history = model_4.fit(train_hybrid_dataset,\n",
    "                             steps_per_epoch=int(0.1 * len(train_hybrid_dataset)),\n",
    "                             epochs=3,\n",
    "                             validation_data=val_hybrid_dataset,\n",
    "                             validation_steps=int(0.1 * len(val_hybrid_dataset)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 83s 139ms/step - loss: 0.9625 - accuracy: 0.6192 - val_loss: 0.7734 - val_accuracy: 0.7055\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 86s 153ms/step - loss: 0.7904 - accuracy: 0.6947 - val_loss: 0.7101 - val_accuracy: 0.7297\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 89s 158ms/step - loss: 0.7670 - accuracy: 0.7061 - val_loss: 0.6782 - val_accuracy: 0.7460\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model Evaluation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "model_4.evaluate(val_hybrid_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "945/945 [==============================] - 27s 28ms/step - loss: 0.6829 - accuracy: 0.7408\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6829282641410828, 0.7407652735710144]"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "model_4_pred_probs = model_4.predict(val_hybrid_dataset)\n",
    "\n",
    "model_4_pred_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4.4185781e-01, 3.2661882e-01, 6.6205263e-03, 2.1611795e-01,\n",
       "        8.7849088e-03],\n",
       "       [3.5951585e-01, 4.5566991e-01, 3.9074398e-03, 1.7916177e-01,\n",
       "        1.7449729e-03],\n",
       "       [3.5247296e-01, 1.6237521e-01, 5.7978321e-02, 3.8545632e-01,\n",
       "        4.1717242e-02],\n",
       "       ...,\n",
       "       [5.9494271e-04, 8.0785053e-03, 7.4097797e-02, 2.9463693e-04,\n",
       "        9.1693407e-01],\n",
       "       [4.6427590e-03, 5.5141047e-02, 2.0368719e-01, 2.3155767e-03,\n",
       "        7.3421335e-01],\n",
       "       [4.4006035e-01, 3.7029508e-01, 1.3380234e-01, 3.1009488e-02,\n",
       "        2.4832774e-02]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
    "model_4_preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 4, 0])>"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_4_preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "all_model_results = pd.DataFrame({\n",
    "    \"baseline\": baseline_results,\n",
    "    \"custom_token_embedding\": model_1_results,\n",
    "    \"pretrained token embedding layer\": model_2_results,\n",
    "    \"Conv1D with character-level embedding\":model_3_results,\n",
    "    \"Token & Char Hybrid Embedding w/ Bi-LSTM\": model_4_results\n",
    "})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           accuracy  precision    recall  \\\n",
       "baseline                                  66.483516   0.668206  0.664835   \n",
       "custom_token_embedding                    78.531709   0.781942  0.785317   \n",
       "pretrained token embedding layer          71.445121   0.714712  0.714451   \n",
       "Conv1D with character-level embedding     65.503773   0.650150  0.655038   \n",
       "Token & Char Hybrid Embedding w/ Bi-LSTM  74.076526   0.739666  0.740765   \n",
       "\n",
       "                                                f1  \n",
       "baseline                                  0.618607  \n",
       "custom_token_embedding                    0.782847  \n",
       "pretrained token embedding layer          0.711513  \n",
       "Conv1D with character-level embedding     0.645507  \n",
       "Token & Char Hybrid Embedding w/ Bi-LSTM  0.738224  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>66.483516</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.618607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_token_embedding</th>\n",
       "      <td>78.531709</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.785317</td>\n",
       "      <td>0.782847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained token embedding layer</th>\n",
       "      <td>71.445121</td>\n",
       "      <td>0.714712</td>\n",
       "      <td>0.714451</td>\n",
       "      <td>0.711513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conv1D with character-level embedding</th>\n",
       "      <td>65.503773</td>\n",
       "      <td>0.650150</td>\n",
       "      <td>0.655038</td>\n",
       "      <td>0.645507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token &amp; Char Hybrid Embedding w/ Bi-LSTM</th>\n",
       "      <td>74.076526</td>\n",
       "      <td>0.739666</td>\n",
       "      <td>0.740765</td>\n",
       "      <td>0.738224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "model_4.save('model_4/saved_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_4/saved_model/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: model_4/saved_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "![title](img/model_5.png) \n",
    "\n",
    "\n",
    "# üõ† Feature Engineering\n",
    "\n",
    "Before we engineer new features, we need to ensure that these features are _available at test time_, meaning, we should be able to obtain these features from new sets of data.\n",
    "\n",
    "* Line number: Easily countable\n",
    "* Total lines: Easily countable as well\n",
    "\n",
    "### Implementing positional embeddings\n",
    "\n",
    "> __Positional embeddings__ indicate where a sentence is in a corpus\n",
    "\n",
    "Since the <b>order</b> in which the sentences appear matter, we can __implement positional embeddings__ which are also available at test time. What i mean is, usually, the classes of the sentences appear in this order {BACKGROUND, OBJECTIVE, METHOD, RESULTS, CONCLUSION}. So, it makes sense to add the line number relative to the number of lines in the abstract.\n",
    "\n",
    "These values are also available at test time (meaning we can just count the no of lines beforehand, since we dont need to label these). We also can't engineer the labels into the training data because they are not available at test time.\n",
    "\n",
    "This is called dimensionality reduction.\n",
    "\n",
    "> One way of creating the positional embedding feature would be to combine the \"line number\" and `total_lines` feature into `line_position` and see how it performs against having two columns instead"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# Our train dataframe already has the data required for creating positional embeddings\n",
    "train_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             target                                               text  \\\n",
       "0         OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
       "1           METHODS  a total of @ patients with primary knee oa wer...   \n",
       "2           METHODS  outcome measures included pain reduction and i...   \n",
       "3           METHODS  pain was assessed using the visual analog pain...   \n",
       "4           METHODS  secondary outcome measures included the wester...   \n",
       "...             ...                                                ...   \n",
       "180035      RESULTS  for the absolute change in percent atheroma vo...   \n",
       "180036      RESULTS  for pav , a significantly greater percentage o...   \n",
       "180037      RESULTS  both strategies had acceptable side effect pro...   \n",
       "180038  CONCLUSIONS  compared with standard statin monotherapy , th...   \n",
       "180039  CONCLUSIONS  ( plaque regression with cholesterol absorptio...   \n",
       "\n",
       "        line_number  total_lines  \n",
       "0                 0           11  \n",
       "1                 1           11  \n",
       "2                 2           11  \n",
       "3                 3           11  \n",
       "4                 4           11  \n",
       "...             ...          ...  \n",
       "180035            7           11  \n",
       "180036            8           11  \n",
       "180037            9           11  \n",
       "180038           10           11  \n",
       "180039           11           11  \n",
       "\n",
       "[180040 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>for the absolute change in percent atheroma vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>for pav , a significantly greater percentage o...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>both strategies had acceptable side effect pro...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>compared with standard statin monotherapy , th...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>( plaque regression with cholesterol absorptio...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows √ó 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the `line_number` and `total_lines` are already numerical, we can pass them to our model. However, to avoid our model from thinking that `line_number = 5` is five times greater than `line_number = 1`, we will one-hot encode our data using the `tf.one_hot` function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "train_df['line_number'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0     15000\n",
       "1     15000\n",
       "2     15000\n",
       "3     15000\n",
       "4     14992\n",
       "5     14949\n",
       "6     14758\n",
       "7     14279\n",
       "8     13346\n",
       "9     11981\n",
       "10    10041\n",
       "11     7892\n",
       "12     5853\n",
       "13     4152\n",
       "14     2835\n",
       "15     1861\n",
       "16     1188\n",
       "17      751\n",
       "18      462\n",
       "19      286\n",
       "20      162\n",
       "21      101\n",
       "22       66\n",
       "23       33\n",
       "24       22\n",
       "25       14\n",
       "26        7\n",
       "27        4\n",
       "28        3\n",
       "29        1\n",
       "30        1\n",
       "Name: line_number, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "train_df.line_number.plot.hist()\n",
    "\n",
    "# We can notice that most of the line numbers lie in the 0~20 lines value."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "metadata": {},
     "execution_count": 99
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# A scalar defining the depth of the one hot dimension.\n",
    "line_number_depth = 20\n",
    "\n",
    "# Use tensorflow to create one-hot-encoded tensors for the line_number column\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df['line_number'].to_numpy(), depth=20)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df['line_number'].to_numpy(), depth=20)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df['line_number'].to_numpy(), depth=20)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Setting the depth parameter of tf.one_hot to 15 means any sample with a \"line_number\" value of over 15 gets set to a tensor of all 0's, where as any sample with a \"line_number\" of under 15 gets turned into a tensor of all 0's but with a 1 at the index equal to the \"line_number\" value.\n",
    "\n",
    "> üîë Note: We could create a one-hot tensor which has room for all of the __potential values__ of \"line_number\" (depth=30), however, this would end up in a tensor of __double__ the size of our current one (depth=15) where the vast majority of values are 0 (sparse). Plus, only ~2,000/180,000 samples have a \"line_number\" value of over 15. So we __would not be gaining much information about our data for doubling our feature space__. This kind of problem is called the _curse of dimensionality_. However, since this we're working with deep models, it might be worth trying to throw as much information at the model as possible and seeing what happens. I'll leave exploring values of the depth parameter as an extension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([180040, 20]),\n",
       " <tf.Tensor: shape=(20, 20), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# Do the same thing, with the total lines\n",
    "train_df['total_lines'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11    24468\n",
       "10    23639\n",
       "12    22113\n",
       "9     19400\n",
       "13    18438\n",
       "14    14610\n",
       "8     12285\n",
       "15    10768\n",
       "7      7464\n",
       "16     7429\n",
       "17     5202\n",
       "6      3353\n",
       "18     3344\n",
       "19     2480\n",
       "20     1281\n",
       "5      1146\n",
       "21      770\n",
       "22      759\n",
       "23      264\n",
       "4       215\n",
       "24      200\n",
       "25      182\n",
       "26       81\n",
       "28       58\n",
       "3        32\n",
       "30       31\n",
       "27       28\n",
       "Name: total_lines, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "train_df['total_lines'].plot.hist()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "metadata": {},
     "execution_count": 103
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "np.percentile(train_df.total_lines, 98)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "total_lines_depth = 20\n",
    "\n",
    "train_total_lines_one_hot = tf.one_hot(train_df['total_lines'].to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df['total_lines'].to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df['total_lines'].to_numpy(), depth=20)\n",
    "\n",
    "train_total_lines_one_hot.shape, train_total_lines_one_hot[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([180040, 20]),\n",
       " <tf.Tensor: shape=(10, 20), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üöÇ Build a input pipeline using the tf.data API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                                  train_total_lines_one_hot,\n",
    "                                                                  train_sentences,\n",
    "                                                                  train_chars))\n",
    "\n",
    "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels))\n",
    "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation dataset\n",
    "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
    "                                                              val_total_lines_one_hot,\n",
    "                                                              val_sentences,\n",
    "                                                              val_chars))\n",
    "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
    "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# 1. Token inputs\n",
    "token_inputs = layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embeddings = universal_sentence_encoder(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation='relu')(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                            outputs=token_outputs)\n",
    "\n",
    "# 2. Char-level embeddings\n",
    "char_inputs = layers.Input(shape=(1,), dtype='string', name='char_inputs')\n",
    "char_tokens = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_tokens)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                           outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Line number inputs\n",
    "line_number_inputs = layers.Input(shape=(line_number_depth,), dtype=tf.int32, name='line_number_inputs')\n",
    "# (1, ) means the length of one array is 15, and there can be as many lists as possible\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                  outputs=x)\n",
    "\n",
    "# 4. Total lines input\n",
    "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name='total_lines_inputs')\n",
    "y = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(inputs=total_lines_inputs,\n",
    "                                 outputs=y)\n",
    "\n",
    "# 5. Combine the token and char embedding layers into hybrid\n",
    "combined_embeddings = layers.Concatenate(name='hybrid_embedding')([token_model.output, char_model.output])\n",
    "\n",
    "z = layers.Dense(256, activation='relu')(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. Combine positional embeddings with the hybrid embedding\n",
    "z = layers.Concatenate(name='hybrid_embedding_with_positional_embedding')([line_number_model.output,\n",
    "                                                                           total_lines_model.output,\n",
    "                                                                           z])\n",
    "\n",
    "# 7. Create output layer\n",
    "output_layer = layers.Dense(num_classes, activation='softmax')(z)\n",
    "\n",
    "# 8. Put the model together\n",
    "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                                total_lines_model.input,\n",
    "                                token_model.input,\n",
    "                                char_model.input],\n",
    "                                outputs=output_layer)\n",
    "\n",
    "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model_5.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_inputs (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_inputs (InputLayer)       [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_vectorizer (TextVectorizat (None, 290)          0           char_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "universal_sentence_encoder (Ker (None, 512)          256797824   token_inputs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, 290, 25)      1700        char_vectorizer[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          65664       universal_sentence_encoder[2][0] \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64)           14848       char_embed[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "hybrid_embedding (Concatenate)  (None, 192)          0           dense_7[0][0]                    \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "line_number_inputs (InputLayer) [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "total_lines_inputs (InputLayer) [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          49408       hybrid_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           672         line_number_inputs[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           672         total_lines_inputs[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hybrid_embedding_with_positiona (None, 320)          0           dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 5)            1605        hybrid_embedding_with_positional_\n",
      "==================================================================================================\n",
      "Total params: 256,932,393\n",
      "Trainable params: 134,569\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "model_5_history = model_5.fit(train_pos_char_token_dataset,\n",
    "                             steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n",
    "                             epochs=3,\n",
    "                             validation_data=val_pos_char_token_dataset,\n",
    "                             validation_steps=int(0.1 * len(val_pos_char_token_dataset)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 102s 173ms/step - loss: 1.0939 - accuracy: 0.7264 - val_loss: 0.9772 - val_accuracy: 0.8115\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 105s 187ms/step - loss: 0.9637 - accuracy: 0.8174 - val_loss: 0.9479 - val_accuracy: 0.8305\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 138s 246ms/step - loss: 0.9454 - accuracy: 0.8278 - val_loss: 0.9370 - val_accuracy: 0.8351\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model Evaluation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "model_5.evaluate(val_pos_char_token_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "945/945 [==============================] - 34s 36ms/step - loss: 0.9339 - accuracy: 0.8348\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9339170455932617, 0.8348338603973389]"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "model_5_pred_probs = model_5.predict(val_pos_char_token_dataset)\n",
    "model_5_pred_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.48283643, 0.11300842, 0.01364156, 0.3734241 , 0.01708954],\n",
       "       [0.5287185 , 0.09684604, 0.04386057, 0.31951505, 0.01105994],\n",
       "       [0.26551786, 0.12391909, 0.10964373, 0.43731368, 0.06360567],\n",
       "       ...,\n",
       "       [0.03657788, 0.12549873, 0.03669219, 0.02967492, 0.77155626],\n",
       "       [0.03268503, 0.306673  , 0.07613857, 0.02811945, 0.55638397],\n",
       "       [0.2535387 , 0.5518059 , 0.08979826, 0.04996707, 0.05489003]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "model_5_preds = tf.argmax(model_5_pred_probs,axis=1)\n",
    "model_5_preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 4, 1])>"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "model_5_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                   y_pred=model_5_preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "all_model_results = pd.DataFrame({\n",
    "    \"baseline\": baseline_results,\n",
    "    \"custom_token_embedding\": model_1_results,\n",
    "    \"pretrained token embedding layer\": model_2_results,\n",
    "    \"Conv1D with character-level embedding\":model_3_results,\n",
    "    \"Token & Char Hybrid Embedding w/ Bi-LSTM\": model_4_results,\n",
    "    \"Token & Char & Positional Embedding w/ Bi-LSTM\": model_5_results\n",
    "})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 accuracy  precision  \\\n",
       "baseline                                        66.483516   0.668206   \n",
       "custom_token_embedding                          78.531709   0.781942   \n",
       "pretrained token embedding layer                71.445121   0.714712   \n",
       "Conv1D with character-level embedding           65.503773   0.650150   \n",
       "Token & Char Hybrid Embedding w/ Bi-LSTM        74.076526   0.739666   \n",
       "Token & Char & Positional Embedding w/ Bi-LSTM  83.483384   0.833833   \n",
       "\n",
       "                                                  recall        f1  \n",
       "baseline                                        0.664835  0.618607  \n",
       "custom_token_embedding                          0.785317  0.782847  \n",
       "pretrained token embedding layer                0.714451  0.711513  \n",
       "Conv1D with character-level embedding           0.655038  0.645507  \n",
       "Token & Char Hybrid Embedding w/ Bi-LSTM        0.740765  0.738224  \n",
       "Token & Char & Positional Embedding w/ Bi-LSTM  0.834834  0.833971  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>66.483516</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.618607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_token_embedding</th>\n",
       "      <td>78.531709</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.785317</td>\n",
       "      <td>0.782847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained token embedding layer</th>\n",
       "      <td>71.445121</td>\n",
       "      <td>0.714712</td>\n",
       "      <td>0.714451</td>\n",
       "      <td>0.711513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conv1D with character-level embedding</th>\n",
       "      <td>65.503773</td>\n",
       "      <td>0.650150</td>\n",
       "      <td>0.655038</td>\n",
       "      <td>0.645507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token &amp; Char Hybrid Embedding w/ Bi-LSTM</th>\n",
       "      <td>74.076526</td>\n",
       "      <td>0.739666</td>\n",
       "      <td>0.740765</td>\n",
       "      <td>0.738224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token &amp; Char &amp; Positional Embedding w/ Bi-LSTM</th>\n",
       "      <td>83.483384</td>\n",
       "      <td>0.833833</td>\n",
       "      <td>0.834834</td>\n",
       "      <td>0.833971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![title](img/conclusion.png) \n",
    "\n",
    "### The tribrid model performs the best by a fair margin\n",
    "\n",
    "Although under performing compared to the model shown in the paper, the model is only trained on 10% of the data.\n",
    "\n",
    "We can upscale this model to get better scores.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "# Reduce the accuracy to the same scale as others\n",
    "all_model_results['accuracy'] = all_model_results['accuracy']/100\n",
    "\n",
    "# Plot and compare the model results on a bar chart\n",
    "all_model_results.plot(kind='bar', figsize=(10,7)).legend(bbox_to_anchor=(1.0, 1.0));"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "# Plot the f1 results alone\n",
    "all_model_results.sort_values('f1', ascending=False)['f1'].plot(kind='bar', figsize=(10,7))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 116
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "model_5.save('model_5/saved_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_5/saved_model/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: model_5/saved_model/assets\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçüèΩ Model 5 Evaluation on the Test dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "test_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,\n",
    "                                                             test_total_lines_one_hot,\n",
    "                                                             test_sentences,\n",
    "                                                             test_chars))\n",
    "\n",
    "test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "\n",
    "test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels))\n",
    "\n",
    "test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_pos_char_token_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 20), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "test_pred_probs = model_5.predict(test_pos_char_token_dataset,\n",
    "                                 verbose=1)\n",
    "\n",
    "test_preds = tf.argmax(test_pred_probs, axis=1) # returns index in that sample's probability array\n",
    "test_preds[:10]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "942/942 [==============================] - 25s 27ms/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([3, 0, 2, 2, 4, 4, 4, 1, 4, 0])>"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "test_results = calculate_results(y_true=test_labels_encoded,\n",
    "                                y_pred=test_preds)\n",
    "test_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 82.9732868757259,\n",
       " 'precision': 0.8284115378063048,\n",
       " 'recall': 0.829732868757259,\n",
       " 'f1': 0.828775254489663}"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model has yet to match the performance of the results in the paper (f1 score of 90, while ours is only 82).\n",
    "\n",
    "### Model Improvements that can be made\n",
    "* Train on full dataset\n",
    "* Fine-tune pretrained embeddings\n",
    "\n",
    "\n",
    "## Find the most wrong predictions\n",
    "One of the best ways to investigate where your model is performing badly, is to visualize the \"most wrong\" predictions, meaning ground truth != prediction. \n",
    "\n",
    "The most wrong predictions are samples where the model has made a prediction with a high probability but has gotten it wrong. (it's confident it has the right answer, but is actually wrong).\n",
    "\n",
    "Doing so, will provide us with valuable information on how to further improve the model or fix the labels in the data.\n",
    "\n",
    "The code below will allow us to visualize the most wrong predictions from the test dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "# Convert all our integer based test predictions into their string-based class names\n",
    "# Get a list of the class names of the test predictions\n",
    "test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds]\n",
    "test_pred_classes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['OBJECTIVE',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'OBJECTIVE',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'METHODS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'BACKGROUND',\n",
       " 'BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'CONCLUSIONS',\n",
       " 'CONCLUSIONS',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Add new columns to our test DataFrame, like the following:__\n",
    "* A 'prediction' (string) column containing our tribrid model's prediction for a given sample\n",
    "* A 'pred_prob' (float) column containing our tribrid model's confidence for that given prediction\n",
    "* A 'correct' (bool) column to indicate whether or not the tribrid model's prediction matches the ground truth"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "# Create a prediction-enriched dataframe\n",
    "test_df['prediction'] = test_pred_classes # Creates a column with test prediction class names\n",
    "\n",
    "# Unlike argmax which returns the index, reduce_max returns the maximum prediction probability\n",
    "test_df['pred_prob'] = tf.reduce_max(test_pred_probs, axis=1).numpy()\n",
    "\n",
    "# Create a binary column for whether the prediction matches the ground truth or not\n",
    "test_df['correct?'] = test_df['prediction'] == test_df['target']\n",
    "\n",
    "test_df.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         target                                               text  \\\n",
       "0    BACKGROUND  this study analyzed liver function abnormaliti...   \n",
       "1       RESULTS  a post hoc analysis was conducted with the use...   \n",
       "2       RESULTS  liver function tests ( lfts ) were measured at...   \n",
       "3       RESULTS  survival analyses were used to assess the asso...   \n",
       "4       RESULTS  the percentage of patients with abnormal lfts ...   \n",
       "5       RESULTS  when mean hemodynamic profiles were compared i...   \n",
       "6       RESULTS  multivariable analyses revealed that patients ...   \n",
       "7   CONCLUSIONS  abnormal lfts are common in the adhf populatio...   \n",
       "8   CONCLUSIONS  elevated meld-xi scores are associated with po...   \n",
       "9    BACKGROUND  minimally invasive endovascular aneurysm repai...   \n",
       "10   BACKGROUND  the aim of this study was to analyse the cost-...   \n",
       "11      METHODS  resource use was determined from the amsterdam...   \n",
       "12      METHODS  the analysis was performed from a provider per...   \n",
       "13      METHODS  all costs were calculated as if all patients h...   \n",
       "14      RESULTS            a total of @ patients were randomized .   \n",
       "15      RESULTS  the @-day mortality rate was @ per cent after ...   \n",
       "16      RESULTS  at @months , the total mortality rate for evar...   \n",
       "17      RESULTS  the mean cost difference between evar and or w...   \n",
       "18      RESULTS  the incremental cost-effectiveness ratio per p...   \n",
       "19      RESULTS  there was no significant difference in quality...   \n",
       "\n",
       "    line_number  total_lines   prediction  pred_prob  correct?  \n",
       "0             0            8    OBJECTIVE   0.458921     False  \n",
       "1             1            8   BACKGROUND   0.328757     False  \n",
       "2             2            8      METHODS   0.766513     False  \n",
       "3             3            8      METHODS   0.656556     False  \n",
       "4             4            8      RESULTS   0.752266      True  \n",
       "5             5            8      RESULTS   0.899854      True  \n",
       "6             6            8      RESULTS   0.545928      True  \n",
       "7             7            8  CONCLUSIONS   0.472478      True  \n",
       "8             8            8      RESULTS   0.474701     False  \n",
       "9             0           12   BACKGROUND   0.567185      True  \n",
       "10            1           12    OBJECTIVE   0.450955     False  \n",
       "11            2           12      METHODS   0.522562      True  \n",
       "12            3           12      METHODS   0.870264      True  \n",
       "13            4           12      METHODS   0.581774      True  \n",
       "14            5           12      RESULTS   0.680306      True  \n",
       "15            6           12      RESULTS   0.658132      True  \n",
       "16            7           12      RESULTS   0.855372      True  \n",
       "17            8           12      RESULTS   0.835286      True  \n",
       "18            9           12      RESULTS   0.801560      True  \n",
       "19           10           12      RESULTS   0.745664      True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>correct?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>this study analyzed liver function abnormaliti...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>0.458921</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>a post hoc analysis was conducted with the use...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>0.328757</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>liver function tests ( lfts ) were measured at...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.766513</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>survival analyses were used to assess the asso...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.656556</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the percentage of patients with abnormal lfts ...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>when mean hemodynamic profiles were compared i...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.899854</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>multivariable analyses revealed that patients ...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.545928</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>abnormal lfts are common in the adhf populatio...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>0.472478</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>elevated meld-xi scores are associated with po...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.474701</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>minimally invasive endovascular aneurysm repai...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>0.567185</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>the aim of this study was to analyse the cost-...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>0.450955</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>resource use was determined from the amsterdam...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.522562</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>the analysis was performed from a provider per...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.870264</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>all costs were calculated as if all patients h...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.581774</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>a total of @ patients were randomized .</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.680306</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the @-day mortality rate was @ per cent after ...</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.658132</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>at @months , the total mortality rate for evar...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.855372</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the mean cost difference between evar and or w...</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.835286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the incremental cost-effectiveness ratio per p...</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.801560</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>there was no significant difference in quality...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>0.745664</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Sort the dataframe to show the samples where the pred_prob is highest and prediction was wrong (correct? = False)__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "top_100_wrong = test_df[test_df['correct?'] == False].sort_values('pred_prob', ascending=False)[:100]\n",
    "top_100_wrong"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            target                                               text  \\\n",
       "13874  CONCLUSIONS  symptom outcomes will be assessed and estimate...   \n",
       "16347   BACKGROUND  to evaluate the effects of the lactic acid bac...   \n",
       "29294      RESULTS  baseline measures included sociodemographics ,...   \n",
       "2388       RESULTS  the primary endpoint is the cumulative three-y...   \n",
       "835     BACKGROUND  to assess the temporal patterns of late gastro...   \n",
       "...            ...                                                ...   \n",
       "12456      RESULTS  icd patients were randomized @:@ to automatic ...   \n",
       "7968    BACKGROUND  dpbrn hygienists internet quality improvement ...   \n",
       "18206    OBJECTIVE  in a randomized controlled trial we examined t...   \n",
       "546        RESULTS  primarily , we assessed the difference between...   \n",
       "5394       RESULTS  a randomized , double-blind , crossover study ...   \n",
       "\n",
       "       line_number  total_lines   prediction  pred_prob  correct?  \n",
       "13874            4            6      METHODS   0.953462     False  \n",
       "16347            0           12    OBJECTIVE   0.943132     False  \n",
       "29294            4           13      METHODS   0.928377     False  \n",
       "2388             4           13      METHODS   0.925926     False  \n",
       "835              0           11    OBJECTIVE   0.924290     False  \n",
       "...            ...          ...          ...        ...       ...  \n",
       "12456            1           13      METHODS   0.836775     False  \n",
       "7968            12           12  CONCLUSIONS   0.836494     False  \n",
       "18206            2           12      METHODS   0.835789     False  \n",
       "546              3           13      METHODS   0.835692     False  \n",
       "5394             3           12      METHODS   0.835404     False  \n",
       "\n",
       "[100 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>correct?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13874</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>symptom outcomes will be assessed and estimate...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.953462</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16347</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>to evaluate the effects of the lactic acid bac...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>0.943132</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29294</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>baseline measures included sociodemographics ,...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.928377</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the primary endpoint is the cumulative three-y...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>to assess the temporal patterns of late gastro...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>0.924290</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>icd patients were randomized @:@ to automatic ...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.836775</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>dpbrn hygienists internet quality improvement ...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>0.836494</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18206</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>in a randomized controlled trial we examined t...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.835789</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>primarily , we assessed the difference between...</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.835692</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>a randomized , double-blind , crossover study ...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>0.835404</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 7 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "# Investigate top wrong preds\n",
    "for row in top_100_wrong[0:10].itertuples(): # adjust indexes to view different samples\n",
    "  _, target, text, line_number, total_lines, prediction, pred_prob, _ = row\n",
    "  print(f\"Target: {target}, Pred: {prediction}, Prob: {pred_prob}, Line number: {line_number}, Total lines: {total_lines}\\n\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"-----\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target: CONCLUSIONS, Pred: METHODS, Prob: 0.9534620642662048, Line number: 4, Total lines: 6\n",
      "\n",
      "Text:\n",
      "symptom outcomes will be assessed and estimates of cost-effectiveness made .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: BACKGROUND, Pred: OBJECTIVE, Prob: 0.9431322813034058, Line number: 0, Total lines: 12\n",
      "\n",
      "Text:\n",
      "to evaluate the effects of the lactic acid bacterium lactobacillus salivarius on caries risk factors .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: RESULTS, Pred: METHODS, Prob: 0.9283771514892578, Line number: 4, Total lines: 13\n",
      "\n",
      "Text:\n",
      "baseline measures included sociodemographics , standardized anthropometrics , asthma control test ( act ) , gerd symptom assessment scale , pittsburgh sleep quality index , and berlin questionnaire for sleep apnea .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: RESULTS, Pred: METHODS, Prob: 0.9259259700775146, Line number: 4, Total lines: 13\n",
      "\n",
      "Text:\n",
      "the primary endpoint is the cumulative three-year hiv incidence .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: BACKGROUND, Pred: OBJECTIVE, Prob: 0.9242895841598511, Line number: 0, Total lines: 11\n",
      "\n",
      "Text:\n",
      "to assess the temporal patterns of late gastrointestinal ( gi ) and genitourinary ( gu ) radiotherapy toxicity and resolution rates in a randomised controlled trial ( all-ireland cooperative oncology research group @-@ ) assessing duration of neo-adjuvant ( na ) hormone therapy for localised prostate cancer .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: METHODS, Pred: BACKGROUND, Prob: 0.9237862229347229, Line number: 1, Total lines: 11\n",
      "\n",
      "Text:\n",
      "pretest-posttest .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: METHODS, Pred: OBJECTIVE, Prob: 0.9206204414367676, Line number: 0, Total lines: 7\n",
      "\n",
      "Text:\n",
      "to determine whether the insulin resistance that exists in metabolic syndrome ( mets ) patients is modulated by dietary fat composition .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: BACKGROUND, Pred: METHODS, Prob: 0.9202765822410583, Line number: 5, Total lines: 15\n",
      "\n",
      "Text:\n",
      "an active control arm received salmeterol ( sal ) twice daily .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: OBJECTIVE, Pred: METHODS, Prob: 0.9195663332939148, Line number: 4, Total lines: 27\n",
      "\n",
      "Text:\n",
      "second , we collected patient-reported outcomes for balance confidence and falls control .\n",
      "\n",
      "-----\n",
      "\n",
      "Target: CONCLUSIONS, Pred: BACKGROUND, Prob: 0.9190315008163452, Line number: 18, Total lines: 18\n",
      "\n",
      "Text:\n",
      "nct@ ( clinicaltrials.gov ) .\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing that can be noticed about the wrong predictions is that the many of the samples are labelled incorrectly/labels are ambiguous (e.g a line in a abstract could potentially be labelled OBJECTIVE or BACKGROUND and still make sense).\n",
    "\n",
    "## Active Learning\n",
    "The next step would be to go through the training and test dataset, update the labels and retrain a model. The process of using a model to help improve/investigate your dataset's labels is often referred to as active learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model on the 200k dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "!ls data/pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dev.txt   test.txt  train.txt train.zip\r\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "data_dir_200k = \"data/pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "# Read the lines of the training dataset, into a variable\n",
    "train_lines = get_lines(data_dir_200k + \"train.txt\") \n",
    "\n",
    "# Show the first 10 lines\n",
    "train_lines[:10] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['###24491034\\n',\n",
       " 'BACKGROUND\\tThe emergence of HIV as a chronic condition means that people living with HIV are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .\\n',\n",
       " 'BACKGROUND\\tThis paper describes the design and evaluation of Positive Outlook , an online program aiming to enhance the self-management skills of gay men living with HIV .\\n',\n",
       " 'METHODS\\tThis study is designed as a randomised controlled trial in which men living with HIV in Australia will be assigned to either an intervention group or usual care control group .\\n',\n",
       " \"METHODS\\tThe intervention group will participate in the online group program ` Positive Outlook ' .\\n\",\n",
       " 'METHODS\\tThe program is based on self-efficacy theory and uses a self-management approach to enhance skills , confidence and abilities to manage the psychosocial issues associated with HIV in daily life .\\n',\n",
       " 'METHODS\\tParticipants will access the program for a minimum of @ minutes per week over seven weeks .\\n',\n",
       " 'METHODS\\tPrimary outcomes are domain specific self-efficacy , HIV related quality of life , and outcomes of health education .\\n',\n",
       " 'METHODS\\tSecondary outcomes include : depression , anxiety and stress ; general health and quality of life ; adjustment to HIV ; and social support .\\n',\n",
       " 'METHODS\\tData collection will take place at baseline , completion of the intervention ( or eight weeks post randomisation ) and at @ week follow-up .\\n']"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "%%time\n",
    "\n",
    "# Get the data and preprocess it\n",
    "train_samples_200k = preprocess_text(data_dir_200k + 'train.txt')\n",
    "val_samples_200k = preprocess_text(data_dir_200k + 'dev.txt') # dev is another name for validation\n",
    "test_samples_200k = preprocess_text(data_dir_200k + 'test.txt')\n",
    "print(\"No. of training samples: {} \\nNo. of validation samples: {} \\nNo. of testing samples: {}\"\n",
    "        .format(len(train_samples_200k), len(val_samples_200k), len(test_samples_200k))\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No. of training samples: 2211861 \n",
      "No. of validation samples: 28932 \n",
      "No. of testing samples: 29493\n",
      "CPU times: user 3.71 s, sys: 505 ms, total: 4.22 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Turn dictionaries into pandas DataFrame to better visualize it\n",
    "train_df_200k = pd.DataFrame(train_samples_200k)\n",
    "val_df_200k = pd.DataFrame(val_samples_200k)\n",
    "test_df_200k = pd.DataFrame(test_samples_200k)\n",
    "\n",
    "# Show first 13 lines of data\n",
    "train_df.head(11)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         target                                               text  \\\n",
       "0    BACKGROUND  the emergence of hiv as a chronic condition me...   \n",
       "1    BACKGROUND  this paper describes the design and evaluation...   \n",
       "2       METHODS  this study is designed as a randomised control...   \n",
       "3       METHODS  the intervention group will participate in the...   \n",
       "4       METHODS  the program is based on self-efficacy theory a...   \n",
       "5       METHODS  participants will access the program for a min...   \n",
       "6       METHODS  primary outcomes are domain specific self-effi...   \n",
       "7       METHODS  secondary outcomes include : depression , anxi...   \n",
       "8       METHODS  data collection will take place at baseline , ...   \n",
       "9   CONCLUSIONS  results of the positive outlook study will pro...   \n",
       "10   BACKGROUND                                           actrn@ .   \n",
       "\n",
       "    line_number  total_lines  \n",
       "0             0           10  \n",
       "1             1           10  \n",
       "2             2           10  \n",
       "3             3           10  \n",
       "4             4           10  \n",
       "5             5           10  \n",
       "6             6           10  \n",
       "7             7           10  \n",
       "8             8           10  \n",
       "9             9           10  \n",
       "10           10           10  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>the emergence of hiv as a chronic condition me...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>this paper describes the design and evaluation...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>this study is designed as a randomised control...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>the intervention group will participate in the...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>the program is based on self-efficacy theory a...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>participants will access the program for a min...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>primary outcomes are domain specific self-effi...</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcomes include : depression , anxi...</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>data collection will take place at baseline , ...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>results of the positive outlook study will pro...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>actrn@ .</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make the 200k train token dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "train_sentences_200k = train_df_200k['text'].tolist()\n",
    "val_sentences_200k = val_df_200k['text'].tolist()\n",
    "test_sentences_200k = test_df_200k['text'].tolist()\n",
    "\n",
    "len(train_sentences_200k), len(val_sentences_200k), len(test_sentences_200k),"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2211861, 28932, 29493)"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "train_sentences_200k[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['the emergence of hiv as a chronic condition means that people living with hiv are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .',\n",
       " 'this paper describes the design and evaluation of positive outlook , an online program aiming to enhance the self-management skills of gay men living with hiv .',\n",
       " 'this study is designed as a randomised controlled trial in which men living with hiv in australia will be assigned to either an intervention group or usual care control group .',\n",
       " \"the intervention group will participate in the online group program ` positive outlook ' .\",\n",
       " 'the program is based on self-efficacy theory and uses a self-management approach to enhance skills , confidence and abilities to manage the psychosocial issues associated with hiv in daily life .',\n",
       " 'participants will access the program for a minimum of @ minutes per week over seven weeks .',\n",
       " 'primary outcomes are domain specific self-efficacy , hiv related quality of life , and outcomes of health education .',\n",
       " 'secondary outcomes include : depression , anxiety and stress ; general health and quality of life ; adjustment to hiv ; and social support .',\n",
       " 'data collection will take place at baseline , completion of the intervention ( or eight weeks post randomisation ) and at @ week follow-up .',\n",
       " 'results of the positive outlook study will provide information regarding the effectiveness of online group programs improving health related outcomes for men living with hiv .']"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Engineering to get one-hot encoded labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "train_labels_one_hot_200k = one_hot_encoder.fit_transform(train_df_200k['target'].to_numpy().reshape(-1,1))\n",
    "val_labels_one_hot_200k = one_hot_encoder.fit_transform(val_df_200k['target'].to_numpy().reshape(-1,1))\n",
    "test_labels_one_hot_200k = one_hot_encoder.fit_transform(test_df_200k['target'].to_numpy().reshape(-1,1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "tf.constant(train_labels_one_hot_200k)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2211861, 5), dtype=float64, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])>"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "train_chars_200k = [split_chars(sentence) for sentence in train_sentences_200k]\n",
    "val_chars_200k = [split_chars(sentence) for sentence in val_sentences_200k]\n",
    "test_chars_200k = [split_chars(sentence) for sentence in test_sentences_200k]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "train_df_200k.line_number.plot.hist()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "metadata": {},
     "execution_count": 172
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEDCAYAAAA4FgP0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAReUlEQVR4nO3dfbBdVX3G8e9DIiKIUiW+TIKCNogZX/GKtmpFKjaCElurhdGpOgx0puLo+FKjddDScUbbqfhSnBqVUWkRo1aalliKStVxRBJE0QTRiCiJ1kREEbUg+usfZ0dPL/cmJ3L3Odyzvp+ZO9l77XX2+a3hcJ+7X87aqSokSe3ab9IFSJImyyCQpMYZBJLUOINAkhpnEEhS4wwCSWrcogyCJOcm2ZnkqyP2f26SrUm2JDm/7/okaTHJYvweQZI/AG4GPlBVD9tL35XAeuC4qroxyX2qauc46pSkxWBRHhFU1WeAHw63JXlwkv9MckWSzyY5qtt0GnBOVd3YvdYQkKQhizII5rEOeElVPQZ4JfDOrv1I4Mgkn0tyWZLVE6tQku6Elk66gIWQ5O7A7wMfTrK7+a7dv0uBlcCxwArgM0keXlU/GnOZknSnNBVBwODI5kdV9ag5tm0HvlBVvwC+leTrDIJh0xjrk6Q7rak4NVRVNzH4Jf8cgAw8stt8IYOjAZIcyuBU0bUTKFOS7pR6C4K93eLZ/bJ+e5JtSa5KcvQ+7PuDwOeBhyTZnuRU4HnAqUm+DGwB1nTdLwZuSLIVuBR4VVXdcEfGJknTpLfbR/d2i2eSE4CXACcAjwPeVlWP66UYSdK8ejsimOsWz1nWMAiJqqrLgEOS3L+veiRJc5vkxeLlwPVD69u7tu/N7pjkdOB0gIMOOugxRx111OwukqQ9uOKKK35QVcvm2rYo7hqqqnUMvifAzMxMbd68ecIVSdLikuTb822b5F1DO4DDhtZXdG2SpDGaZBBsAP68u3vo8cCPq+p2p4UkSf3q7dRQd4vnscChSbYDrwfuAlBV/wRsZHDH0DbgZ8CL+qpFkjS/3oKgqk7Zy/YCXtzX+0uSRjMV3yyWJP32DAJJapxBIEmNMwgkqXEGgSQ1blF8s3ihHL72oom993VvOnFi7y1Je+IRgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4pp5ZPEmTel6yz0qWtDceEUhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuN6DYIkq5Nck2RbkrVzbH9AkkuTXJnkqiQn9FmPJOn2eguCJEuAc4CnA6uAU5KsmtXtdcD6qno0cDLwzr7qkSTNrc8jgmOAbVV1bVXdClwArJnVp4B7dMv3BL7bYz2SpDn0GQTLgeuH1rd3bcPeADw/yXZgI/CSuXaU5PQkm5Ns3rVrVx+1SlKzJn2x+BTgfVW1AjgBOC/J7WqqqnVVNVNVM8uWLRt7kZI0zfoMgh3AYUPrK7q2YacC6wGq6vPAAcChPdYkSZqlzyDYBKxMckSS/RlcDN4wq893gD8ESPJQBkHguR9JGqPegqCqbgPOAC4GrmZwd9CWJGclOanr9grgtCRfBj4IvLCqqq+aJEm31+sTyqpqI4OLwMNtZw4tbwWe0GcNkqQ9m/TFYknShBkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUuF6DIMnqJNck2ZZk7Tx9nptka5ItSc7vsx5J0u0t7WvHSZYA5wDHA9uBTUk2VNXWoT4rgdcAT6iqG5Pcp696JElzG+mIIMnDf4t9HwNsq6prq+pW4AJgzaw+pwHnVNWNAFW187d4H0nSHTDqqaF3Jrk8yV8mueeIr1kOXD+0vr1rG3YkcGSSzyW5LMnquXaU5PQkm5Ns3rVr14hvL0kaxUhBUFVPAp4HHAZckeT8JMcvwPsvBVYCxwKnAO9Ocsgc77+uqmaqambZsmUL8LaSpN1GvlhcVd8AXge8Gngy8PYkX0vyJ/O8ZAeD4NhtRdc2bDuwoap+UVXfAr7OIBgkSWMy6jWCRyQ5G7gaOA54ZlU9tFs+e56XbQJWJjkiyf7AycCGWX0uZHA0QJJDGZwqunYfxyBJugNGvWvoHcB7gNdW1c93N1bVd5O8bq4XVNVtSc4ALgaWAOdW1ZYkZwGbq2pDt+1pSbYCvwReVVU33IHxSJL20ahBcCLw86r6JUCS/YADqupnVXXefC+qqo3AxlltZw4tF/Dy7keSNAGjXiP4BHC3ofUDuzZJ0iI3ahAcUFU3717plg/spyRJ0jiNGgQ/TXL07pUkjwF+vof+kqRFYtRrBC8DPpzku0CA+wF/1ldRkqTxGSkIqmpTkqOAh3RN11TVL/orS5I0Lvsy6dxjgcO71xydhKr6QC9VSZLGZqQgSHIe8GDgSwzu9wcowCCQpEVu1COCGWBVd9+/JGmKjHrX0FcZXCCWJE2ZUY8IDgW2JrkcuGV3Y1Wd1EtVkqSxGTUI3tBnEZKkyRn19tFPJ3kgsLKqPpHkQAYTyUmSFrlRp6E+DfgI8K6uaTmDKaQlSYvcqBeLXww8AbgJfv2QGh80L0lTYNQguKV7AD0ASZYy+B6BJGmRGzUIPp3ktcDdumcVfxj49/7KkiSNy6hBsBbYBXwF+AsGD5uZ88lkkqTFZdS7hn4FvLv7kSRNkVHnGvoWc1wTqKoHLXhFkqSx2pe5hnY7AHgOcK+FL0eSNG4jXSOoqhuGfnZU1VsZPNBekrTIjXpq6Oih1f0YHCHsy7MMJEl3UqP+Mv+HoeXbgOuA5y54NZKksRv1rqGn9F2IJGkyRj019PI9ba+qtyxMOZKkcduXu4YeC2zo1p8JXA58o4+iJEnjM2oQrACOrqqfACR5A3BRVT2/r8IkSeMx6hQT9wVuHVq/tWuTJC1yox4RfAC4PMnHuvVnAe/vpSJJ0liNetfQG5N8HHhS1/Siqrqyv7IkSeMy6qkhgAOBm6rqbcD2JEf0VJMkaYxGfVTl64FXA6/pmu4C/HNfRUmSxmfUI4I/Bk4CfgpQVd8FDu6rKEnS+IwaBLdWVdFNRZ3koP5KkiSN06hBsD7Ju4BDkpwGfAIfUiNJU2GvQZAkwIeAjwAfBR4CnFlV7xjhtauTXJNkW5K1e+j37CSVZGa+PpKkfuz19tGqqiQbq+rhwCWj7jjJEuAc4HhgO7ApyYaq2jqr38HAS4Ev7FPlkqQFMeqpoS8meew+7vsYYFtVXVtVtwIXAGvm6Pe3wJuB/93H/UuSFsCoQfA44LIk30xyVZKvJLlqL69ZDlw/tL69a/u17oE3h1XVRXvaUZLTk2xOsnnXrl0jlixJGsUeTw0leUBVfQf4o4V+4yT7AW8BXri3vlW1DlgHMDMzUwtdiyS1bG/XCC5kMOvot5N8tKqevQ/73gEcNrS+omvb7WDgYcB/D65Hcz9gQ5KTqmrzPryPJOkO2NupoQwtP2gf970JWJnkiCT7Ayfzm+cZUFU/rqpDq+rwqjocuAwwBCRpzPYWBDXP8l5V1W3AGcDFwNXA+qrakuSsJCftW5mSpL7s7dTQI5PcxODI4G7dMt16VdU99vTiqtoIbJzVduY8fY8dqWJJ0oLaYxBU1ZJxFSJJmox9mYZakjSFDAJJapxBIEmNMwgkqXEGgSQ1bqSH12vxOnztHqdx6tV1bzpxYu8taXQeEUhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjes1CJKsTnJNkm1J1s6x/eVJtia5Ksknkzywz3okSbfXWxAkWQKcAzwdWAWckmTVrG5XAjNV9QjgI8Df9VWPJGlufR4RHANsq6prq+pW4AJgzXCHqrq0qn7WrV4GrOixHknSHPoMguXA9UPr27u2+ZwKfHyuDUlOT7I5yeZdu3YtYImSpDvFxeIkzwdmgL+fa3tVrauqmaqaWbZs2XiLk6Qpt7THfe8ADhtaX9G1/T9Jngr8NfDkqrqlx3okSXPo84hgE7AyyRFJ9gdOBjYMd0jyaOBdwElVtbPHWiRJ8+gtCKrqNuAM4GLgamB9VW1JclaSk7pufw/cHfhwki8l2TDP7iRJPenz1BBVtRHYOKvtzKHlp/b5/pKkvbtTXCyWJE2OQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNW7ppAvQ9Dp87UUTed/r3nTiRN5XWqw8IpCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmN6zUIkqxOck2SbUnWzrH9rkk+1G3/QpLD+6xHknR7vQVBkiXAOcDTgVXAKUlWzep2KnBjVf0ucDbw5r7qkSTNrc8vlB0DbKuqawGSXACsAbYO9VkDvKFb/gjwj0lSVdVjXZpyfpFN2jd9BsFy4Pqh9e3A4+brU1W3JfkxcG/gB8OdkpwOnN6t3pzkmt+ypkNn77sBjnlMMtnjWf87t+GOjPmB821YFFNMVNU6YN0d3U+SzVU1swAlLRqOuQ2OuQ19jbnPi8U7gMOG1ld0bXP2SbIUuCdwQ481SZJm6TMINgErkxyRZH/gZGDDrD4bgBd0y38KfMrrA5I0Xr2dGurO+Z8BXAwsAc6tqi1JzgI2V9UG4L3AeUm2AT9kEBZ9usOnlxYhx9wGx9yGXsYc/wCXpLb5zWJJapxBIEmNayYI9jbdxTRIcm6SnUm+OtR2rySXJPlG9+/vTLLGhZTksCSXJtmaZEuSl3bt0zzmA5JcnuTL3Zj/pms/opumZVs3bcv+k651oSVZkuTKJP/RrU/1mJNcl+QrSb6UZHPX1stnu4kgGHG6i2nwPmD1rLa1wCeraiXwyW59WtwGvKKqVgGPB17c/Xed5jHfAhxXVY8EHgWsTvJ4BtOznN1N13Ijg+lbps1LgauH1lsY81Oq6lFD3x3o5bPdRBAwNN1FVd0K7J7uYqpU1WcY3H01bA3w/m75/cCzxllTn6rqe1X1xW75Jwx+SSxnusdcVXVzt3qX7qeA4xhM0wJTNmaAJCuAE4H3dOthysc8j14+260EwVzTXSyfUC3jdt+q+l63/D/AfSdZTF+6mWsfDXyBKR9zd4rkS8BO4BLgm8CPquq2rss0fr7fCvwV8Ktu/d5M/5gL+K8kV3TT7EBPn+1FMcWEFkZVVZKpu184yd2BjwIvq6qbBn8sDkzjmKvql8CjkhwCfAw4arIV9SvJM4CdVXVFkmMnXM44PbGqdiS5D3BJkq8Nb1zIz3YrRwSjTHcxrb6f5P4A3b87J1zPgkpyFwYh8C9V9a9d81SPebeq+hFwKfB7wCHdNC0wfZ/vJwAnJbmOwWnd44C3Md1jpqp2dP/uZBD4x9DTZ7uVIBhluotpNTyNxwuAf5tgLQuqO0/8XuDqqnrL0KZpHvOy7kiAJHcDjmdwbeRSBtO0wJSNuapeU1UrqupwBv/vfqqqnscUjznJQUkO3r0MPA34Kj19tpv5ZnGSExicZ9w93cUbJ1vRwkvyQeBYBlPVfh94PXAhsB54APBt4LlVNfuC8qKU5InAZ4Gv8Jtzx69lcJ1gWsf8CAYXCZcw+ENufVWdleRBDP5avhdwJfD8qrplcpX2ozs19MqqesY0j7kb28e61aXA+VX1xiT3pofPdjNBIEmaWyunhiRJ8zAIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuP+DxA33MOaxRVFAAAAAElFTkSuQmCC"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "# Since we can notice most of the line numbers lie in the 0 - 20 line value, we set the depth to be 20\n",
    "\n",
    "line_number_depth=20\n",
    "\n",
    "# Use tensorflow to create one-hot-encoded tensors for the line_number column\n",
    "train_line_numbers_one_hot_200k = tf.one_hot(train_df_200k['line_number'].to_numpy(), depth=line_number_depth)\n",
    "val_line_numbers_one_hot_200k = tf.one_hot(val_df_200k['line_number'].to_numpy(), depth=line_number_depth)\n",
    "test_line_numbers_one_hot_200k = tf.one_hot(test_df_200k['line_number'].to_numpy(), depth=line_number_depth)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "train_line_numbers_one_hot_200k.shape, train_line_numbers_one_hot_200k[:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([2211861, 20]),\n",
       " <tf.Tensor: shape=(20, 20), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "# Do the same thing with total lines feature\n",
    "np.percentile(train_df_200k.total_lines, 98)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "train_df_200k['total_lines'].plot.hist()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "metadata": {},
     "execution_count": 176
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "total_lines_depth = 20\n",
    "\n",
    "train_total_lines_one_hot_200k = tf.one_hot(train_df_200k['total_lines'].to_numpy(), depth=total_lines_depth)\n",
    "val_total_lines_one_hot_200k = tf.one_hot(val_df_200k['total_lines'].to_numpy(), depth=total_lines_depth)\n",
    "test_total_lines_one_hot_200k = tf.one_hot(test_df_200k['total_lines'].to_numpy(), depth=total_lines_depth)\n",
    "\n",
    "train_total_lines_one_hot_200k.shape, train_total_lines_one_hot_200k[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([2211861, 20]),\n",
       " <tf.Tensor: shape=(10, 20), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üöÇ Build a input pipeline using the tf.data API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "tribrid_model_train_data_200k = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot_200k,\n",
    "                                                                  train_total_lines_one_hot_200k,\n",
    "                                                                  train_sentences_200k,\n",
    "                                                                  train_chars_200k))\n",
    "\n",
    "tribrid_model_train_labels_200k = tf.data.Dataset.from_tensor_slices(train_labels_one_hot_200k)\n",
    "tribrid_model_train_dataset_200k = tf.data.Dataset.zip((tribrid_model_train_data_200k, tribrid_model_train_labels_200k))\n",
    "tribrid_model_train_dataset_200k = tribrid_model_train_dataset_200k.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation dataset\n",
    "tribrid_model_val_data_200k = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot_200k,\n",
    "                                                              val_total_lines_one_hot_200k,\n",
    "                                                              val_sentences_200k,\n",
    "                                                              val_chars_200k))\n",
    "tribrid_model_val_labels_200k = tf.data.Dataset.from_tensor_slices(val_labels_one_hot_200k)\n",
    "tribrid_model_val_dataset_200k = tf.data.Dataset.zip((tribrid_model_val_data_200k, tribrid_model_val_labels_200k))\n",
    "tribrid_model_val_dataset_200k = tribrid_model_val_dataset_200k.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "tribrid_model_train_dataset_200k, tribrid_model_val_dataset_200k"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 20), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>,\n",
       " <PrefetchDataset shapes: (((None, 20), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>)"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "model_5_history = model_5.fit(tribrid_model_train_dataset_200k,\n",
    "                             steps_per_epoch=int(0.1 * len(tribrid_model_train_dataset_200k)),\n",
    "                             epochs=3,\n",
    "                             validation_data=tribrid_model_val_dataset_200k,\n",
    "                             validation_steps=int(0.1 * len(tribrid_model_val_dataset_200k)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "6912/6912 [==============================] - 1785s 258ms/step - loss: 0.9173 - accuracy: 0.8462 - val_loss: 0.9062 - val_accuracy: 0.8517\n",
      "Epoch 2/3\n",
      "6912/6912 [==============================] - 1880s 272ms/step - loss: 0.9043 - accuracy: 0.8549 - val_loss: 0.8956 - val_accuracy: 0.8632\n",
      "Epoch 3/3\n",
      "6912/6912 [==============================] - 1529s 221ms/step - loss: 0.8995 - accuracy: 0.8584 - val_loss: 0.8885 - val_accuracy: 0.8628\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "model_5.save('model_5/tribrid_200k_saved_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model_5/tribrid_200k_saved_model/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: model_5/tribrid_200k_saved_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "# Download and open example abstracts (copy and pasted from PubMed)\n",
    "import json\n",
    "\n",
    "with open(\"data/skimlit_example_abstracts.json\", \"r\") as f:\n",
    "  example_abstracts = json.load(f)\n",
    "\n",
    "example_abstracts"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'abstract': 'This RCT examined the efficacy of a manualized social intervention for children with HFASDs. Participants were randomly assigned to treatment or wait-list conditions. Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language. A response-cost program was applied to reduce problem behaviors and foster skills acquisition. Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures). Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents. High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity. Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.',\n",
       "  'source': 'https://pubmed.ncbi.nlm.nih.gov/20232240/',\n",
       "  'details': 'RCT of a manualized social treatment for high-functioning autism spectrum disorders'},\n",
       " {'abstract': \"Postpartum depression (PPD) is the most prevalent mood disorder associated with childbirth. No single cause of PPD has been identified, however the increased risk of nutritional deficiencies incurred through the high nutritional requirements of pregnancy may play a role in the pathology of depressive symptoms. Three nutritional interventions have drawn particular interest as possible non-invasive and cost-effective prevention and/or treatment strategies for PPD; omega-3 (n-3) long chain polyunsaturated fatty acids (LCPUFA), vitamin D and overall diet. We searched for meta-analyses of randomised controlled trials (RCT's) of nutritional interventions during the perinatal period with PPD as an outcome, and checked for any trials published subsequently to the meta-analyses. Fish oil: Eleven RCT's of prenatal fish oil supplementation RCT's show null and positive effects on PPD symptoms. Vitamin D: no relevant RCT's were identified, however seven observational studies of maternal vitamin D levels with PPD outcomes showed inconsistent associations. Diet: Two Australian RCT's with dietary advice interventions in pregnancy had a positive and null result on PPD. With the exception of fish oil, few RCT's with nutritional interventions during pregnancy assess PPD. Further research is needed to determine whether nutritional intervention strategies during pregnancy can protect against symptoms of PPD. Given the prevalence of PPD and ease of administering PPD measures, we recommend future prenatal nutritional RCT's include PPD as an outcome.\",\n",
       "  'source': 'https://pubmed.ncbi.nlm.nih.gov/28012571/',\n",
       "  'details': 'Formatting removed (can be used to compare model to actual example)'},\n",
       " {'abstract': 'Mental illness, including depression, anxiety and bipolar disorder, accounts for a significant proportion of global disability and poses a substantial social, economic and heath burden. Treatment is presently dominated by pharmacotherapy, such as antidepressants, and psychotherapy, such as cognitive behavioural therapy; however, such treatments avert less than half of the disease burden, suggesting that additional strategies are needed to prevent and treat mental disorders. There are now consistent mechanistic, observational and interventional data to suggest diet quality may be a modifiable risk factor for mental illness. This review provides an overview of the nutritional psychiatry field. It includes a discussion of the neurobiological mechanisms likely modulated by diet, the use of dietary and nutraceutical interventions in mental disorders, and recommendations for further research. Potential biological pathways related to mental disorders include inflammation, oxidative stress, the gut microbiome, epigenetic modifications and neuroplasticity. Consistent epidemiological evidence, particularly for depression, suggests an association between measures of diet quality and mental health, across multiple populations and age groups; these do not appear to be explained by other demographic, lifestyle factors or reverse causality. Our recently published intervention trial provides preliminary clinical evidence that dietary interventions in clinically diagnosed populations are feasible and can provide significant clinical benefit. Furthermore, nutraceuticals including n-3 fatty acids, folate, S-adenosylmethionine, N-acetyl cysteine and probiotics, among others, are promising avenues for future research. Continued research is now required to investigate the efficacy of intervention studies in large cohorts and within clinically relevant populations, particularly in patients with schizophrenia, bipolar and anxiety disorders.',\n",
       "  'source': 'https://pubmed.ncbi.nlm.nih.gov/28942748/',\n",
       "  'details': 'Effect of nutrition on mental health'},\n",
       " {'abstract': \"Hepatitis C virus (HCV) and alcoholic liver disease (ALD), either alone or in combination, count for more than two thirds of all liver diseases in the Western world. There is no safe level of drinking in HCV-infected patients and the most effective goal for these patients is total abstinence. Baclofen, a GABA(B) receptor agonist, represents a promising pharmacotherapy for alcohol dependence (AD). Previously, we performed a randomized clinical trial (RCT), which demonstrated the safety and efficacy of baclofen in patients affected by AD and cirrhosis. The goal of this post-hoc analysis was to explore baclofen's effect in a subgroup of alcohol-dependent HCV-infected cirrhotic patients. Any patient with HCV infection was selected for this analysis. Among the 84 subjects randomized in the main trial, 24 alcohol-dependent cirrhotic patients had a HCV infection; 12 received baclofen 10mg t.i.d. and 12 received placebo for 12-weeks. With respect to the placebo group (3/12, 25.0%), a significantly higher number of patients who achieved and maintained total alcohol abstinence was found in the baclofen group (10/12, 83.3%; p=0.0123). Furthermore, in the baclofen group, compared to placebo, there was a significantly higher increase in albumin values from baseline (p=0.0132) and a trend toward a significant reduction in INR levels from baseline (p=0.0716). In conclusion, baclofen was safe and significantly more effective than placebo in promoting alcohol abstinence, and improving some Liver Function Tests (LFTs) (i.e. albumin, INR) in alcohol-dependent HCV-infected cirrhotic patients. Baclofen may represent a clinically relevant alcohol pharmacotherapy for these patients.\",\n",
       "  'source': 'https://pubmed.ncbi.nlm.nih.gov/22244707/',\n",
       "  'details': 'Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection'}]"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "# See what our example abstracts look like\n",
    "abstracts = pd.DataFrame(example_abstracts)\n",
    "abstracts"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            abstract  \\\n",
       "0  This RCT examined the efficacy of a manualized...   \n",
       "1  Postpartum depression (PPD) is the most preval...   \n",
       "2  Mental illness, including depression, anxiety ...   \n",
       "3  Hepatitis C virus (HCV) and alcoholic liver di...   \n",
       "\n",
       "                                      source  \\\n",
       "0  https://pubmed.ncbi.nlm.nih.gov/20232240/   \n",
       "1  https://pubmed.ncbi.nlm.nih.gov/28012571/   \n",
       "2  https://pubmed.ncbi.nlm.nih.gov/28942748/   \n",
       "3  https://pubmed.ncbi.nlm.nih.gov/22244707/   \n",
       "\n",
       "                                             details  \n",
       "0  RCT of a manualized social treatment for high-...  \n",
       "1  Formatting removed (can be used to compare mod...  \n",
       "2               Effect of nutrition on mental health  \n",
       "3  Baclofen promotes alcohol abstinence in alcoho...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>source</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This RCT examined the efficacy of a manualized...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/20232240/</td>\n",
       "      <td>RCT of a manualized social treatment for high-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Postpartum depression (PPD) is the most preval...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/28012571/</td>\n",
       "      <td>Formatting removed (can be used to compare mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mental illness, including depression, anxiety ...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/28942748/</td>\n",
       "      <td>Effect of nutrition on mental health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hepatitis C virus (HCV) and alcoholic liver di...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/22244707/</td>\n",
       "      <td>Baclofen promotes alcohol abstinence in alcoho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "# Create sentencizer - Source: https://spacy.io/usage/linguistic-features#sbd \n",
    "from spacy.lang.en import English\n",
    "nlp = English() # setup English sentence parser\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\") # create sentence splitting pipeline object\n",
    "nlp.add_pipe(\"sentencizer\") # add sentence splitting pipeline object to sentence parser\n",
    "doc = nlp(example_abstracts[0][\"abstract\"]) # create \"doc\" of parsed sequences, change index for a different abstract\n",
    "abstract_lines = [str(sent) for sent in list(doc.sents)] # return detected sentences from doc in string type (not spaCy token type)\n",
    "abstract_lines"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This RCT examined the efficacy of a manualized social intervention for children with HFASDs.',\n",
       " 'Participants were randomly assigned to treatment or wait-list conditions.',\n",
       " 'Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.',\n",
       " 'A response-cost program was applied to reduce problem behaviors and foster skills acquisition.',\n",
       " 'Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).',\n",
       " 'Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.',\n",
       " 'High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.',\n",
       " 'Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.']"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "# Get total number of lines\n",
    "total_lines_in_sample = len(abstract_lines)\n",
    "\n",
    "# Go through each line in abstract and create a list of dictionaries containing features for each line\n",
    "sample_lines = []\n",
    "for i, line in enumerate(abstract_lines):\n",
    "  sample_dict = {}\n",
    "  sample_dict[\"text\"] = str(line)\n",
    "  sample_dict[\"line_number\"] = i\n",
    "  sample_dict[\"total_lines\"] = total_lines_in_sample - 1\n",
    "  sample_lines.append(sample_dict)\n",
    "sample_lines"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'text': 'This RCT examined the efficacy of a manualized social intervention for children with HFASDs.',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 7},\n",
       " {'text': 'Participants were randomly assigned to treatment or wait-list conditions.',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 7},\n",
       " {'text': 'Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 7},\n",
       " {'text': 'A response-cost program was applied to reduce problem behaviors and foster skills acquisition.',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 7},\n",
       " {'text': 'Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 7},\n",
       " {'text': 'Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 7},\n",
       " {'text': 'High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 7},\n",
       " {'text': 'Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 7}]"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "# Get all line_number values from sample abstract\n",
    "test_abstract_line_numbers = [line[\"line_number\"] for line in sample_lines]\n",
    "# One-hot encode to same depth as training data, so model accepts right input shape\n",
    "test_abstract_line_numbers_one_hot = tf.one_hot(test_abstract_line_numbers, depth=20) \n",
    "test_abstract_line_numbers_one_hot"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 20), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "# Get all total_lines values from sample abstract\n",
    "test_abstract_total_lines = [line[\"total_lines\"] for line in sample_lines]\n",
    "# One-hot encode to same depth as training data, so model accepts right input shape\n",
    "test_abstract_total_lines_one_hot = tf.one_hot(test_abstract_total_lines, depth=20)\n",
    "test_abstract_total_lines_one_hot"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 20), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "# Split abstract lines into characters\n",
    "abstract_chars = [split_chars(sentence) for sentence in abstract_lines]\n",
    "abstract_chars"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['T h i s   R C T   e x a m i n e d   t h e   e f f i c a c y   o f   a   m a n u a l i z e d   s o c i a l   i n t e r v e n t i o n   f o r   c h i l d r e n   w i t h   H F A S D s .',\n",
       " 'P a r t i c i p a n t s   w e r e   r a n d o m l y   a s s i g n e d   t o   t r e a t m e n t   o r   w a i t - l i s t   c o n d i t i o n s .',\n",
       " 'T r e a t m e n t   i n c l u d e d   i n s t r u c t i o n   a n d   t h e r a p e u t i c   a c t i v i t i e s   t a r g e t i n g   s o c i a l   s k i l l s ,   f a c e - e m o t i o n   r e c o g n i t i o n ,   i n t e r e s t   e x p a n s i o n ,   a n d   i n t e r p r e t a t i o n   o f   n o n - l i t e r a l   l a n g u a g e .',\n",
       " 'A   r e s p o n s e - c o s t   p r o g r a m   w a s   a p p l i e d   t o   r e d u c e   p r o b l e m   b e h a v i o r s   a n d   f o s t e r   s k i l l s   a c q u i s i t i o n .',\n",
       " 'S i g n i f i c a n t   t r e a t m e n t   e f f e c t s   w e r e   f o u n d   f o r   f i v e   o f   s e v e n   p r i m a r y   o u t c o m e   m e a s u r e s   ( p a r e n t   r a t i n g s   a n d   d i r e c t   c h i l d   m e a s u r e s ) .',\n",
       " 'S e c o n d a r y   m e a s u r e s   b a s e d   o n   s t a f f   r a t i n g s   ( t r e a t m e n t   g r o u p   o n l y )   c o r r o b o r a t e d   g a i n s   r e p o r t e d   b y   p a r e n t s .',\n",
       " 'H i g h   l e v e l s   o f   p a r e n t ,   c h i l d   a n d   s t a f f   s a t i s f a c t i o n   w e r e   r e p o r t e d ,   a l o n g   w i t h   h i g h   l e v e l s   o f   t r e a t m e n t   f i d e l i t y .',\n",
       " 'S t a n d a r d i z e d   e f f e c t   s i z e   e s t i m a t e s   w e r e   p r i m a r i l y   i n   t h e   m e d i u m   a n d   l a r g e   r a n g e s   a n d   f a v o r e d   t h e   t r e a t m e n t   g r o u p .']"
      ]
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "# Make predictions on sample abstract features\n",
    "test_abstract_pred_probs = model_5.predict(x=(test_abstract_line_numbers_one_hot,\n",
    "                                                   test_abstract_total_lines_one_hot,\n",
    "                                                   tf.constant(abstract_lines),\n",
    "                                                   tf.constant(abstract_chars)))\n",
    "test_abstract_pred_probs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.44440904, 0.0895683 , 0.02664932, 0.3944996 , 0.04487377],\n",
       "       [0.12836225, 0.04965863, 0.65367866, 0.0987638 , 0.06953672],\n",
       "       [0.14011009, 0.06320566, 0.63707995, 0.10588949, 0.0537148 ],\n",
       "       [0.11617265, 0.14510854, 0.504328  , 0.08285461, 0.1515362 ],\n",
       "       [0.05102038, 0.0866999 , 0.38781208, 0.04537955, 0.42908806],\n",
       "       [0.05262087, 0.05790544, 0.64530575, 0.0463637 , 0.19780424],\n",
       "       [0.02565636, 0.11159401, 0.0903073 , 0.02222847, 0.75021386],\n",
       "       [0.02028968, 0.10956138, 0.09573721, 0.02413413, 0.75027764]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "# Turn prediction probabilities into prediction classes\n",
    "test_abstract_preds = tf.argmax(test_abstract_pred_probs, axis=1)\n",
    "test_abstract_preds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([0, 2, 2, 2, 4, 2, 4, 4])>"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we've got the predicted sequence label for each line in our sample abstract, let's write some code to visualize each sentence with its predicted label.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "# Turn prediction class integers into string class names\n",
    "test_abstract_pred_classes = [label_encoder.classes_[i] for i in test_abstract_preds]\n",
    "test_abstract_pred_classes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['BACKGROUND',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS']"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "# Visualize abstract lines and predicted sequence labels\n",
    "for i, line in enumerate(abstract_lines):\n",
    "  print(f\"{test_abstract_pred_classes[i]}: {line}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BACKGROUND: This RCT examined the efficacy of a manualized social intervention for children with HFASDs.\n",
      "METHODS: Participants were randomly assigned to treatment or wait-list conditions.\n",
      "METHODS: Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.\n",
      "METHODS: A response-cost program was applied to reduce problem behaviors and foster skills acquisition.\n",
      "RESULTS: Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).\n",
      "METHODS: Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.\n",
      "RESULTS: High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.\n",
      "RESULTS: Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice! Isn't that much easier to read? I mean, it looks like our model's predictions could be improved, but how cool is that?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üí° Ideas\n",
    "\n",
    "* Implement the model on the backend of the PubMed website to format any unstructured RCT abstract on the site\n",
    "* Chrome browser extension that reads the HTML data of a RCT url and add structure to the abstract using the trained model\n",
    "    * Allow users to correct any incorrect labels. Perhaps a \"Is this label accurate?\" popup when hovering over the abstract.\n",
    "* Functionizing the abstract preprocessing pipeline instead of having to manually convert the data to a df, splicing, one hot encoding, finding the depth, etc etc.\n",
    "* Merge the line_number and total_lines feature into one feature.\n",
    "* Replace the TFHUB Sentence Encoder with TFHub BERT PubMed expert (a language model pretrained on pubmed texts) pretrained embedding.\n",
    "* Train the model_5 for many epochs and use modelcheckpoints and earlystopping callbacks."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbee7842ce8ba476870a006d5d5b68f11cea175afb0fea017b7f81beccf88892"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}